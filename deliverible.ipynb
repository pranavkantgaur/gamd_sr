{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkantgaur/gamd_sr/blob/main/deliverible.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "1. Demonstrate the existence and merits of algorithm alignment in GNNs for physical simulations, if any.\n",
        "2. Keep abstractions limited to experimental data and stable python libraries.\n",
        "\n",
        "## Tasks\n",
        "1. Loads a dataset from LJ, tip3w, tip4w, DFT\n",
        "2. Optionally, generates MD dataset by passing a custom potential energy expressions as input\n",
        "3. Loads relevant GNN model based on dataset complexity\n",
        "4. Trains GNN and records edge messages of the converged model\n",
        "5. Test for linearity of fit between pair potential and pair-force vs edge messages\n",
        "6. Depending on the underlying custom potential, defines SR inputs and outputs\n",
        "7. Trains SR\n",
        "8. Plots pred vs gt curve for best SR equation\n",
        "9. Discusses algorithm alignment of GNNs with the underlying physical phenomenon algorithm.\n",
        "10. Perhaps demomstrates the benefits of algo. alignment:\n",
        "   1. Whether algo. alignment is natural in GNNs or it happens becuase of the inductive biases?"
      ],
      "metadata": {
        "id": "zBDelF3uS7n6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "XfsBHVY0TzZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessed dataset sources\n",
        "1. LJ test-cases: https://drive.google.com/file/d/1jJdTAnhps1EIHDaBfb893fruaLPJzYKI/view?usp=sharing\n",
        "2. Tip-3P test-cases: https://drive.google.com/file/d/18uvKVtN8Xm_5w7AJuezFiR1d2AqlHFKn/view?usp=sharing\n",
        "3. Tip-4p test-cases: https://drive.google.com/file/d/1jBk78hN4ZPC9x-YXnznUzxFnXnpeKFRi/view?usp=sharing\n",
        "4. DFT test-cases: https://drive.google.com/file/d/1b9P7EvIliGupN9ZIJpMGZzkm4ttG9Ul6/view?usp=sharing"
      ],
      "metadata": {
        "id": "YCi_RwSe8WoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset classes and dataloaders"
      ],
      "metadata": {
        "id": "_AsGRi5e83GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class WaterDataNew(Dataset):\n",
        "    def __init__(self,\n",
        "                 dataset_path,\n",
        "                 sample_num,   # per seed\n",
        "                 case_prefix='data_',\n",
        "                 seed_num=10,\n",
        "                 m_num=258,    # tip3p 258, tip4p 251\n",
        "                 split=(0.9, 0.1),\n",
        "                 mode='train',\n",
        "                 data_type='tip3p',\n",
        "                 ):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.sample_num = sample_num\n",
        "        self.case_prefix = case_prefix\n",
        "        self.seed_num = seed_num\n",
        "\n",
        "        self.data_type = data_type\n",
        "        particle_type = []\n",
        "        for i in range(m_num * 3):\n",
        "            particle_type.append(1 if i % 3 == 0 else 0)\n",
        "        self.particle_type = np.array(particle_type).astype(np.int64).reshape(-1, 1)\n",
        "        # transform into one hot encoding\n",
        "        self.particle_type_one_hot = np.zeros((self.particle_type.size, 1), dtype=np.float32)\n",
        "        self.particle_type_one_hot[self.particle_type.reshape(-1) == 1] = 1\n",
        "        self.num_atom_type = self.particle_type.max() + 1\n",
        "        print(f'Including atom type: {self.num_atom_type}')\n",
        "\n",
        "        self.mode = mode\n",
        "        assert mode in ['train', 'test']\n",
        "        idxs = np.arange(seed_num*sample_num)\n",
        "        np.random.seed(0)   # fix same random seed\n",
        "        np.random.shuffle(idxs)\n",
        "        ratio = split[0]\n",
        "        if mode == 'train':\n",
        "            self.idx = idxs[:int(len(idxs)*ratio)]\n",
        "        else:\n",
        "            self.idx = idxs[int(len(idxs)*ratio):]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "    def __getitem__(self, idx, get_path_name=False):\n",
        "        idx = self.idx[idx]\n",
        "        sample_to_read = idx % self.sample_num\n",
        "        seed = idx // self.sample_num\n",
        "        fname = f'data_{seed}_{sample_to_read}'#f'seed_{seed_to_read}_data_{sample_to_read}'\n",
        "        data_path = os.path.join(self.dataset_path, fname)\n",
        "\n",
        "        data = {}\n",
        "        with np.load(data_path + '.npz', 'rb') as raw_data:\n",
        "            pos = raw_data['pos'].astype(np.float32)\n",
        "            if self.data_type == 'tip4p':\n",
        "                pos = pos[np.mod(np.arange(pos.shape[0]), 4) < 3]\n",
        "            data['pos'] = pos\n",
        "            data['feat'] = self.particle_type_one_hot\n",
        "            forces = raw_data['forces'].astype(np.float32)\n",
        "            if self.data_type == 'tip4p':\n",
        "                forces = forces[np.mod(np.arange(forces.shape[0]), 4) < 3]\n",
        "            data['forces'] = forces\n",
        "        if get_path_name:\n",
        "            return data, data_path\n",
        "        return data\n",
        "\n",
        "\n",
        "class LJDataNew(Dataset):\n",
        "    def __init__(self,\n",
        "                 dataset_path,\n",
        "                 sample_num,   # per seed\n",
        "                 case_prefix='ljdata_',\n",
        "                 seed_num=10,\n",
        "                 split=(0.9, 0.1),\n",
        "                 mode='train',\n",
        "                 ):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.sample_num = sample_num\n",
        "        self.case_prefix = case_prefix\n",
        "        self.seed_num = seed_num\n",
        "\n",
        "        self.mode = mode\n",
        "        assert mode in ['train', 'test']\n",
        "        idxs = np.arange(seed_num*sample_num)\n",
        "        np.random.seed(0)   # fix same random seed\n",
        "        np.random.shuffle(idxs)\n",
        "        ratio = split[0]\n",
        "        if mode == 'train':\n",
        "            self.idx = idxs[:int(len(idxs)*ratio)]\n",
        "        else:\n",
        "            self.idx = idxs[int(len(idxs)*ratio):]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "    def __getitem__(self, idx, get_path_name=False):\n",
        "        idx = self.idx[idx]\n",
        "        #sample_to_read = 999 + idx % self.sample_num\n",
        "        sample_to_read = idx % self.sample_num\n",
        "        seed = idx // self.sample_num\n",
        "        '''\n",
        "        if sample_to_read < 999:\n",
        "          print(\"Error\")\n",
        "          exit(0)\n",
        "        '''\n",
        "        fname = f'ljdata_{seed}_{sample_to_read}'\n",
        "        data_path = os.path.join(self.dataset_path, fname)\n",
        "\n",
        "        data = {}\n",
        "        with np.load(data_path + '.npz', 'rb') as raw_data:\n",
        "            pos = raw_data['pos'].astype(np.float32)\n",
        "            data['pos'] = pos\n",
        "            forces = raw_data['forces'].astype(np.float32)\n",
        "            data['forces'] = forces\n",
        "        if get_path_name:\n",
        "            return data, data_path\n",
        "        return data\n",
        "\n",
        "\n",
        "class WaterDataRealLarge(Dataset):\n",
        "    def __init__(self,\n",
        "                 dataset_path,\n",
        "                 mode='train',\n",
        "                 use_part=False\n",
        "                 ):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.use_part = use_part\n",
        "        with np.load(self.dataset_path, allow_pickle=True) as npz_data:\n",
        "            train_idx = npz_data['train_idx']\n",
        "            test_idx = npz_data['test_idx']\n",
        "            self.pos = npz_data['pos']\n",
        "            self.forces = npz_data['force']\n",
        "            self.box_size = npz_data['box']\n",
        "            self.atom_type = npz_data['atom_type']\n",
        "        if use_part:\n",
        "            print(f'Using 1500 training samples')\n",
        "        else:\n",
        "            print(f'Using {len(train_idx)} training samples')\n",
        "        print(f'Using {len(test_idx)} testing samples')\n",
        "\n",
        "        if mode == 'train':\n",
        "            if not use_part:\n",
        "                self.idx = train_idx\n",
        "            else:\n",
        "                self.idx = train_idx[:1500]\n",
        "        else:\n",
        "            self.idx = test_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "    def generate_atom_emb(self, particle_type):\n",
        "        particle_type = np.array(particle_type).astype(np.int64).reshape(-1, 1)\n",
        "        # transform into one hot encoding\n",
        "        particle_type_one_hot = np.zeros((particle_type.size, 1), dtype=np.float32)\n",
        "        particle_type_one_hot[particle_type.reshape(-1) == 1] = 1\n",
        "        return particle_type_one_hot\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = {}\n",
        "        data['pos'] = self.pos[self.idx[idx]].copy().astype(np.float32)\n",
        "        data['feat'] = self.generate_atom_emb(self.atom_type[self.idx[idx]])\n",
        "        data['forces'] = self.forces[self.idx[idx]].copy().astype(np.float32)\n",
        "        data['box_size'] = self.box_size[self.idx[idx]].copy().astype(np.float32)\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "XYhdH1BdzLFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For LJ system"
      ],
      "metadata": {
        "id": "XPUVg1bqz05G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = LJDataNew(dataset_path=os.path.join(self.data_dir, ''),\n",
        "                        sample_num=1000,\n",
        "                        case_prefix='data_',\n",
        "                        seed_num=10,\n",
        "                        mode='train')\n",
        "\n",
        "dataloader = DataLoader(dataset, num_workers=2, batch_size=self.batch_size, shuffle=True,\n",
        "                  collate_fn=\n",
        "                  lambda batches: {\n",
        "                      'pos': [batch['pos'] for batch in batches],\n",
        "                      'forces': [batch['forces'] for batch in batches],\n",
        "                  })"
      ],
      "metadata": {
        "id": "ZzFMVD3Ezh4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For tip3p system"
      ],
      "metadata": {
        "id": "7eTtMh5dz385"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = WaterDataNew(dataset_path=os.path.join(self.data_dir, 'water_data_tip3p'),\n",
        "                        sample_num=1000,\n",
        "                        case_prefix='data_',\n",
        "                        seed_num=10,\n",
        "                        m_num=NUM_OF_ATOMS//3,\n",
        "                        mode='train',\n",
        "                        data_type='tip3p')\n",
        "\n",
        "dataloader = DataLoader(dataset, num_workers=2, batch_size=self.batch_size, shuffle=True,\n",
        "                  collate_fn=\n",
        "                  lambda batches: {\n",
        "                      'feat': torch.cat([torch.from_numpy(batch['feat']).float() for batch in batches], dim=0),\n",
        "                      'pos': [batch['pos'] for batch in batches],\n",
        "                      'forces': [batch['forces'] for batch in batches],\n",
        "                  })\n"
      ],
      "metadata": {
        "id": "jedSxnEN0C01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Tip4p system"
      ],
      "metadata": {
        "id": "ekkDjN4g0Ah8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = WaterDataNew(dataset_path=os.path.join(self.data_dir, 'water_data_tip4p'),\n",
        "                        sample_num=1000,\n",
        "                        case_prefix='data_',\n",
        "                        seed_num=10,\n",
        "                        m_num=NUM_OF_ATOMS//3,\n",
        "                        mode='train',\n",
        "                        data_type='tip4p')\n",
        "\n",
        "dataloader = DataLoader(dataset, num_workers=2, batch_size=self.batch_size, shuffle=True,\n",
        "                  collate_fn=\n",
        "                  lambda batches: {\n",
        "                      'feat': torch.cat([torch.from_numpy(batch['feat']).float() for batch in batches], dim=0),\n",
        "                      'pos': [batch['pos'] for batch in batches],\n",
        "                      'forces': [batch['forces'] for batch in batches],\n",
        "                  })\n"
      ],
      "metadata": {
        "id": "ADnGHaCiz6Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For DFT system"
      ],
      "metadata": {
        "id": "7JlFcEg30yD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = WaterDataRealLarge(dataset_path=os.path.join(self.data_dir, 'RPBE-data-processed.npz'), use_part=self.use_part)\n",
        "return DataLoader(dataset, num_workers=2, batch_size=self.batch_size, shuffle=True,\n",
        "                  collate_fn=\n",
        "                  lambda batches: {\n",
        "                      'feat': torch.cat([torch.from_numpy(batch['feat']).float() for batch in batches], dim=0),\n",
        "                      'pos': [batch['pos'] for batch in batches],\n",
        "                      'forces': [batch['forces'] for batch in batches],\n",
        "                      'box_size': [batch['box_size'] for batch in batches],\n",
        "                  })"
      ],
      "metadata": {
        "id": "1b5B2CLA00f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optionally, generate MD dataset"
      ],
      "metadata": {
        "id": "o5zDEG7-T3o0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate LJ test-cases"
      ],
      "metadata": {
        "id": "R1InqRF875U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import openmm as mm\n",
        "from openmm import *\n",
        "from openmm.app import *\n",
        "from openmm.unit import *\n",
        "from openmm import unit\n",
        "from openmm import app\n",
        "\n",
        "from openmmtools import integrators\n",
        "from openmmtools import testsystems\n",
        "\n",
        "\n",
        "def subrandom_particle_positions(nparticles, box_vectors, method='sobol'):\n",
        "    \"\"\"Generate a deterministic list of subrandom particle positions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    nparticles : int\n",
        "        The number of particles.\n",
        "    box_vectors : openmm.unit.Quantity of (3,3) with units compatible with nanometer\n",
        "        Periodic box vectors in which particles should lie.\n",
        "    method : str, optional, default='sobol'\n",
        "        Method for creating subrandom sequence (one of 'halton' or 'sobol')\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    positions : openmm.unit.Quantity of (natoms,3) with units compatible with nanometer\n",
        "        The particle positions.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> nparticles = 216\n",
        "    >>> box_vectors = openmm.System().getDefaultPeriodicBoxVectors()\n",
        "    >>> positions = subrandom_particle_positions(nparticles, box_vectors)\n",
        "\n",
        "    Use halton sequence:\n",
        "\n",
        "    >>> nparticles = 216\n",
        "    >>> box_vectors = openmm.System().getDefaultPeriodicBoxVectors()\n",
        "    >>> positions = subrandom_particle_positions(nparticles, box_vectors, method='halton')\n",
        "\n",
        "    \"\"\"\n",
        "    # Create positions array.\n",
        "    positions = unit.Quantity(np.zeros([nparticles, 3], np.float32), unit.nanometers)\n",
        "\n",
        "    if method == 'halton':\n",
        "        # Fill in each dimension.\n",
        "        primes = [2, 3, 5]  # prime bases for Halton sequence\n",
        "        for dim in range(3):\n",
        "            x = halton_sequence(primes[dim], nparticles)\n",
        "            l = box_vectors[dim][dim]\n",
        "            positions[:, dim] = unit.Quantity(x * l / l.unit, l.unit)\n",
        "\n",
        "    elif method == 'sobol':\n",
        "        # Generate Sobol' sequence.\n",
        "        from openmmtools import sobol\n",
        "        ivec = sobol.i4_sobol_generate(3, nparticles, 1)\n",
        "        x = np.array(ivec, np.float32)\n",
        "        for dim in range(3):\n",
        "            l = box_vectors[dim][dim]\n",
        "            positions[:, dim] = unit.Quantity(x[dim, :] * l / l.unit, l.unit)\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"method '%s' must be 'halton' or 'sobol'\" % method)\n",
        "\n",
        "    return positions\n",
        "\n",
        "\n",
        "\n",
        "def get_rotation_matrix():\n",
        "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
        "        rotation is per shape based along up direction\n",
        "        Input:\n",
        "          Nx3 array, original point clouds\n",
        "        Return:\n",
        "          Nx3 array, rotated point clouds\n",
        "    \"\"\"\n",
        "    angles = np.random.uniform(-1.0, 1.0, size=(3,)) * np.pi\n",
        "    print(f'Using angle: {angles}')\n",
        "    Rx = np.array([[1., 0, 0],\n",
        "                       [0, np.cos(angles[0]), -np.sin(angles[0])],\n",
        "                       [0, np.sin(angles[0]), np.cos(angles[0])]], dtype=np.float32)\n",
        "    Ry = np.array([[np.cos(angles[1]), 0, np.sin(angles[1])],\n",
        "                       [0, 1, 0],\n",
        "                       [-np.sin(angles[1]), 0, np.cos(angles[1])]], dtype=np.float32)\n",
        "    Rz = np.array([[np.cos(angles[2]), -np.sin(angles[2]), 0],\n",
        "                   [np.sin(angles[2]), np.cos(angles[2]), 0],\n",
        "                   [0, 0, 1]], dtype=np.float32)\n",
        "    rotation_matrix = np.matmul(Rz, np.matmul(Ry, Rx))\n",
        "\n",
        "    return rotation_matrix\n",
        "\n",
        "def center_positions(pos):\n",
        "    offset = np.mean(pos, axis=0)\n",
        "    return pos - offset, offset\n",
        "\n",
        "\n",
        "BOX_SCALE = 2\n",
        "DT = 2\n",
        "for seed in range(10):\n",
        "    print(f'Running seed: {seed}')\n",
        "    nparticles = 50\n",
        "\n",
        "    reduced_density=0.05\n",
        "    mass=39.9 * unit.amu  # argon\n",
        "    sigma=3.4 * unit.angstrom  # argon,\n",
        "    epsilon=0.238 * unit.kilocalories_per_mole  # argon,\n",
        "    cutoff = 3.0 * sigma\n",
        "    mass=39.9 * unit.amu\n",
        "    charge = 0.0 * unit.elementary_charge\n",
        "    # Create an empty system object.\n",
        "    system = openmm.System()\n",
        "\n",
        "    # Determine volume and periodic box vectors.\n",
        "    number_density = reduced_density / sigma**3\n",
        "    volume = nparticles * (number_density ** -1)\n",
        "    box_edge = volume ** (1. / 3.)\n",
        "    a = unit.Quantity((box_edge,        0 * unit.angstrom, 0 * unit.angstrom))\n",
        "    b = unit.Quantity((0 * unit.angstrom, box_edge,        0 * unit.angstrom))\n",
        "    c = unit.Quantity((0 * unit.angstrom, 0 * unit.angstrom, box_edge))\n",
        "    system.setDefaultPeriodicBoxVectors(a, b, c)\n",
        "\n",
        "    # Define Lennard-Jones potential with periodic boundary conditions (global constants)\n",
        "    lj_potential = '4*epsilon*((sigma/r)^12 - (sigma/r)^6)'  # Using epsilon and sigma as constants in the expression\n",
        "    custom_force = CustomNonbondedForce(lj_potential)\n",
        "    # Add the constants for epsilon and sigma as global parameters\n",
        "    custom_force.addGlobalParameter(\"epsilon\", epsilon)\n",
        "    custom_force.addGlobalParameter(\"sigma\", sigma)\n",
        "\n",
        "    # Set periodic cutoff for nonbonded interactions\n",
        "    custom_force.setNonbondedMethod(CustomNonbondedForce.CutoffPeriodic)\n",
        "    custom_force.setCutoffDistance(cutoff)\n",
        "\n",
        "\n",
        "    for particle_index in range(nparticles):\n",
        "      system.addParticle(mass)\n",
        "      custom_force.addParticle([])\n",
        "\n",
        "    system.addForce(custom_force)\n",
        "\n",
        "\n",
        "    # Define initial positions\n",
        "    positions = subrandom_particle_positions(nparticles, system.getDefaultPeriodicBoxVectors())\n",
        "\n",
        "    # Create topology.\n",
        "    topology = app.Topology()\n",
        "    element = app.Element.getBySymbol('Ar')\n",
        "    chain = topology.addChain()\n",
        "    for particle in range(system.getNumParticles()):\n",
        "        residue = topology.addResidue('Ar', chain)\n",
        "        topology.addAtom('Ar', element, residue)\n",
        "    topology = topology\n",
        "\n",
        "\n",
        "    R = get_rotation_matrix()\n",
        "    positions = positions.value_in_unit(unit.angstrom)\n",
        "    positions, off = center_positions(positions)\n",
        "    positions = np.matmul(positions, R)\n",
        "    positions += off\n",
        "    positions += np.random.randn(positions.shape[0], positions.shape[1]) * 0.005\n",
        "    positions *= unit.angstrom\n",
        "\n",
        "    timestep = DT * unit.femtoseconds\n",
        "    temperature = 100 * unit.kelvin\n",
        "    chain_length = 10\n",
        "    friction = 25. / unit.picosecond\n",
        "    num_mts = 5\n",
        "    num_yoshidasuzuki = 5\n",
        "\n",
        "    integrator1 = integrators.NoseHooverChainVelocityVerletIntegrator(system,\n",
        "                                                                      temperature,\n",
        "                                                                      friction,\n",
        "                                                                      timestep, chain_length, num_mts, num_yoshidasuzuki)\n",
        "\n",
        "\n",
        "    platform = Platform.getPlatformByName(\"CUDA\")\n",
        "    platformProperties = {'Precision': 'mixed', 'DeviceIndex': '0, 1, 2'}\n",
        "\n",
        "\n",
        "    simulation = Simulation(topology, system, integrator1, platform, platformProperties)\n",
        "\n",
        "    simulation.context.setPositions(positions)\n",
        "    simulation.context.setVelocitiesToTemperature(temperature)\n",
        "\n",
        "    simulation.minimizeEnergy(tolerance=1*unit.kilojoule/(unit.mole*unit.nanometer))\n",
        "    simulation.step(1)\n",
        "\n",
        "    os.makedirs(f'./lj_data_ours/run_7', exist_ok=True)\n",
        "    stepsPerIter = 5000\n",
        "    totalIter = 2000\n",
        "    totalSteps = stepsPerIter * totalIter\n",
        "    dataReporter_gt = StateDataReporter(f'./lj_data_ours/run_7/log_nvt_lj_{seed}.txt', stepsPerIter, totalSteps=totalSteps,\n",
        "        step=True, time=True, speed=True, progress=True, elapsedTime=True, remainingTime=True,\n",
        "        potentialEnergy=True, kineticEnergy=True, totalEnergy=True, temperature=True,\n",
        "                                     separator='\\t')\n",
        "    simulation.reporters.append(dataReporter_gt)\n",
        "\n",
        "    for t in range(totalIter):\n",
        "        #if (t+1)%100 == 0:\n",
        "        #    print(f'Finished {(t+1)*stepsPerIter} steps')\n",
        "        state = simulation.context.getState(getPositions=True,\n",
        "                                             getVelocities=True,\n",
        "                                             getForces=True,\n",
        "                                             enforcePeriodicBox=True)\n",
        "        pos = state.getPositions(asNumpy=True).value_in_unit(unit.angstrom)\n",
        "        vel = state.getVelocities(asNumpy=True).value_in_unit(unit.meter / unit.second)\n",
        "        force = state.getForces(asNumpy=True).value_in_unit(unit.kilojoules_per_mole/unit.nanometer)\n",
        "        np.savez(f'./lj_data_ours/run_7/data_{seed}_{t}.npz',\n",
        "                 pos=pos,\n",
        "                 vel=vel,\n",
        "                 forces=force)\n",
        "        simulation.step(stepsPerIter)\n",
        "\n"
      ],
      "metadata": {
        "id": "PYen61vt0976"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate tip-3p test-cases"
      ],
      "metadata": {
        "id": "uLhN3Y5v8AC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openmmtools import testsystems\n",
        "from simtk.openmm.app import *\n",
        "import simtk.unit as unit\n",
        "\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from openmmtools.constants import kB\n",
        "from openmmtools import respa, utils\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Energy unit used by OpenMM unit system\n",
        "from openmmtools import states, integrators\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "def get_rotation_matrix():\n",
        "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
        "        rotation is per shape based along up direction\n",
        "        Input:\n",
        "          Nx3 array, original point clouds\n",
        "        Return:\n",
        "          Nx3 array, rotated point clouds\n",
        "    \"\"\"\n",
        "    angles = np.random.uniform(-1.0, 1.0, size=(3,)) * np.pi\n",
        "    print(f'Using angle: {angles}')\n",
        "    Rx = np.array([[1., 0, 0],\n",
        "                       [0, np.cos(angles[0]), -np.sin(angles[0])],\n",
        "                       [0, np.sin(angles[0]), np.cos(angles[0])]], dtype=np.float32)\n",
        "    Ry = np.array([[np.cos(angles[1]), 0, np.sin(angles[1])],\n",
        "                       [0, 1, 0],\n",
        "                       [-np.sin(angles[1]), 0, np.cos(angles[1])]], dtype=np.float32)\n",
        "    Rz = np.array([[np.cos(angles[2]), -np.sin(angles[2]), 0],\n",
        "                   [np.sin(angles[2]), np.cos(angles[2]), 0],\n",
        "                   [0, 0, 1]], dtype=np.float32)\n",
        "    rotation_matrix = np.matmul(Rz, np.matmul(Ry, Rx))\n",
        "\n",
        "    return rotation_matrix\n",
        "\n",
        "def center_positions(pos):\n",
        "    offset = np.mean(pos, axis=0)\n",
        "    return pos - offset, offset\n",
        "\n",
        "\n",
        "BOX_SCALE = 2\n",
        "DT = 2\n",
        "for seed in range(10):\n",
        "    print(f'Running seed: {seed}')\n",
        "\n",
        "    waterbox = testsystems.WaterBox(\n",
        "        box_edge=2 * unit.nanometers,\n",
        "        model='tip3p')\n",
        "    [topology, system, positions] = [waterbox.topology, waterbox.system, waterbox.positions]\n",
        "\n",
        "    R = get_rotation_matrix()\n",
        "    positions = positions.value_in_unit(unit.angstrom)\n",
        "    positions, off = center_positions(positions)\n",
        "    positions = np.matmul(positions, R)\n",
        "    positions += off\n",
        "    positions += np.random.randn(positions.shape[0], positions.shape[1]) * 0.005\n",
        "    positions *= unit.angstrom\n",
        "\n",
        "    p_num = positions.shape[0] // 3\n",
        "    timestep = DT * unit.femtoseconds\n",
        "    temperature = 300 * unit.kelvin\n",
        "    chain_length = 10\n",
        "    friction = 1. / unit.picosecond\n",
        "    num_mts = 5\n",
        "    num_yoshidasuzuki = 5\n",
        "\n",
        "    integrator = integrators.NoseHooverChainVelocityVerletIntegrator(system,\n",
        "                                                                      temperature,\n",
        "                                                                      friction,\n",
        "                                                                      timestep, chain_length, num_mts, num_yoshidasuzuki)\n",
        "\n",
        "    simulation = Simulation(topology, system, integrator)\n",
        "    simulation.context.setPositions(positions)\n",
        "    simulation.context.setVelocitiesToTemperature(temperature)\n",
        "\n",
        "    simulation.minimizeEnergy(tolerance=1*unit.kilojoule/unit.mole)\n",
        "    simulation.step(1)\n",
        "\n",
        "    os.makedirs(f'./water_data_tip3p/', exist_ok=True)\n",
        "    dataReporter_gt = StateDataReporter(f'./log_nvt_tip3p_{seed}.txt', 50, totalSteps=50000,\n",
        "        step=True, time=True, speed=True, progress=True, elapsedTime=True, remainingTime=True,\n",
        "        potentialEnergy=True, kineticEnergy=True, totalEnergy=True, temperature=True,\n",
        "                                     separator='\\t')\n",
        "    simulation.reporters.append(dataReporter_gt)\n",
        "    for t in range(1000):\n",
        "        if (t+1)%100 == 0:\n",
        "            print(f'Finished {(t+1)*50} steps')\n",
        "        state = simulation.context.getState(getPositions=True,\n",
        "                                             getVelocities=True,\n",
        "                                             getForces=True,\n",
        "                                             enforcePeriodicBox=True)\n",
        "        pos = state.getPositions(asNumpy=True).value_in_unit(unit.angstrom)\n",
        "        vel = state.getVelocities(asNumpy=True).value_in_unit(unit.meter / unit.second)\n",
        "        force = state.getForces(asNumpy=True).value_in_unit(unit.kilojoules_per_mole/unit.nanometer)\n",
        "\n",
        "        np.savez(f'./water_data_tip3p/data_{seed}_{t}.npz',\n",
        "                 pos=pos,\n",
        "                 vel=vel,\n",
        "                 forces=force)\n",
        "        simulation.step(50)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wm_VCyLD8EYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate tip-4p test-cases"
      ],
      "metadata": {
        "id": "dLZd31BS8FEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openmmtools import testsystems\n",
        "from simtk.openmm.app import *\n",
        "import simtk.unit as unit\n",
        "\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from openmmtools.constants import kB\n",
        "from openmmtools import respa, utils\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Energy unit used by OpenMM unit system\n",
        "from openmmtools import states, integrators\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "def get_rotation_matrix():\n",
        "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
        "        rotation is per shape based along up direction\n",
        "        Input:\n",
        "          Nx3 array, original point clouds\n",
        "        Return:\n",
        "          Nx3 array, rotated point clouds\n",
        "    \"\"\"\n",
        "    angles = np.random.uniform(-1.0, 1.0, size=(3,)) * np.pi\n",
        "    print(f'Using angle: {angles}')\n",
        "    Rx = np.array([[1., 0, 0],\n",
        "                       [0, np.cos(angles[0]), -np.sin(angles[0])],\n",
        "                       [0, np.sin(angles[0]), np.cos(angles[0])]], dtype=np.float32)\n",
        "    Ry = np.array([[np.cos(angles[1]), 0, np.sin(angles[1])],\n",
        "                       [0, 1, 0],\n",
        "                       [-np.sin(angles[1]), 0, np.cos(angles[1])]], dtype=np.float32)\n",
        "    Rz = np.array([[np.cos(angles[2]), -np.sin(angles[2]), 0],\n",
        "                   [np.sin(angles[2]), np.cos(angles[2]), 0],\n",
        "                   [0, 0, 1]], dtype=np.float32)\n",
        "    rotation_matrix = np.matmul(Rz, np.matmul(Ry, Rx))\n",
        "\n",
        "    return rotation_matrix\n",
        "\n",
        "def center_positions(pos):\n",
        "    offset = np.mean(pos, axis=0)\n",
        "    return pos - offset, offset\n",
        "\n",
        "\n",
        "BOX_SCALE = 2\n",
        "DT = 2\n",
        "for seed in range(10):\n",
        "    print(f'Running seed: {seed}')\n",
        "\n",
        "    waterbox = testsystems.WaterBox(\n",
        "        box_edge=2 * unit.nanometers,\n",
        "        model='tip4pew')\n",
        "    [topology, system, positions] = [waterbox.topology, waterbox.system, waterbox.positions]\n",
        "\n",
        "    R = get_rotation_matrix()\n",
        "    positions = positions.value_in_unit(unit.angstrom)\n",
        "    positions, off = center_positions(positions)\n",
        "    positions = np.matmul(positions, R)\n",
        "    positions += off\n",
        "    positions += np.random.randn(positions.shape[0], positions.shape[1]) * 0.005\n",
        "    positions *= unit.angstrom\n",
        "\n",
        "    p_num = positions.shape[0] // 3\n",
        "    timestep = DT * unit.femtoseconds\n",
        "    temperature = 300 * unit.kelvin\n",
        "    chain_length = 10\n",
        "    friction = 1. / unit.picosecond\n",
        "    num_mts = 5\n",
        "    num_yoshidasuzuki = 5\n",
        "\n",
        "    integrator = integrators.NoseHooverChainVelocityVerletIntegrator(system,\n",
        "                                                                      temperature,\n",
        "                                                                      friction,\n",
        "                                                                      timestep, chain_length, num_mts, num_yoshidasuzuki)\n",
        "\n",
        "    simulation = Simulation(topology, system, integrator)\n",
        "    simulation.context.setPositions(positions)\n",
        "    simulation.context.setVelocitiesToTemperature(temperature)\n",
        "\n",
        "    simulation.minimizeEnergy(tolerance=1*unit.kilojoule/unit.mole)\n",
        "    simulation.step(1)\n",
        "\n",
        "    os.makedirs(f'./water_data_tip4p/', exist_ok=True)\n",
        "    dataReporter_gt = StateDataReporter(f'./log_nvt_tip4p_{seed}.txt', 50, totalSteps=50000,\n",
        "        step=True, time=True, speed=True, progress=True, elapsedTime=True, remainingTime=True,\n",
        "        potentialEnergy=True, kineticEnergy=True, totalEnergy=True, temperature=True,\n",
        "                                     separator='\\t')\n",
        "    simulation.reporters.append(dataReporter_gt)\n",
        "    for t in range(1000):\n",
        "        if (t+1)%100 == 0:\n",
        "            print(f'Finished {(t+1)*50} steps')\n",
        "        state = simulation.context.getState(getPositions=True,\n",
        "                                             getVelocities=True,\n",
        "                                             getForces=True,\n",
        "                                             enforcePeriodicBox=True)\n",
        "        pos = state.getPositions(asNumpy=True).value_in_unit(unit.angstrom)\n",
        "        vel = state.getVelocities(asNumpy=True).value_in_unit(unit.meter / unit.second)\n",
        "        force = state.getForces(asNumpy=True).value_in_unit(unit.kilojoules_per_mole/unit.nanometer)\n",
        "\n",
        "        np.savez(f'./water_data_tip4p/data_{seed}_{t}.npz',\n",
        "                 pos=pos,\n",
        "                 vel=vel,\n",
        "                 forces=force)\n",
        "        simulation.step(50)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iH2M-MjF8G99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize the MD simulation dataset"
      ],
      "metadata": {
        "id": "Vc7jxBcG6w8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### .npz to .pdb file conversion"
      ],
      "metadata": {
        "id": "Ns9LMB3a7S1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set the input directory containing your .npz files\n",
        "input_directory = '/home/pranav/gamd_sr/openmm_data_generation/lj_data_ours/run_5/'\n",
        "# Set the output directory where you want to save .pdb files\n",
        "output_directory = 'top_pymol_ours'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Loop through all .npz files in the input directory\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith('.npz'):\n",
        "        # Extract simulation_id and frame_id from the filename\n",
        "        parts = filename.split('_')\n",
        "        if len(parts) != 3:\n",
        "            continue  # Skip files that do not match the expected naming convention\n",
        "\n",
        "        simulation_id = parts[1]  # Get the simulation ID\n",
        "        frame_id = parts[2].replace('.npz', '')  # Get the frame ID without extension\n",
        "\n",
        "        # Load the .npz file\n",
        "        data = np.load(os.path.join(input_directory, filename))\n",
        "        positions = data['pos']  # Assuming 'pos' is the key for positions\n",
        "\n",
        "        # Create a subdirectory for each simulation if it doesn't exist\n",
        "        sim_output_dir = os.path.join(output_directory, f'simulation_{simulation_id}')\n",
        "        os.makedirs(sim_output_dir, exist_ok=True)\n",
        "\n",
        "        # Save positions to a .pdb file in the corresponding subdirectory\n",
        "        pdb_filename = f'frame_{frame_id}.pdb'\n",
        "        with open(os.path.join(sim_output_dir, pdb_filename), 'w') as f:\n",
        "            for i, pos in enumerate(positions):\n",
        "                f.write(f\"ATOM  {i+1:5d}  Ar   RES A   1    {pos[0]:8.3f}{pos[1]:8.3f}{pos[2]:8.3f}\\n\")\n",
        "            f.write(\"END\\n\")\n",
        "\n",
        "print(\"Conversion complete.\")\n"
      ],
      "metadata": {
        "id": "ipeA0R2I7doW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### .pdb to .mpg movie file conversion"
      ],
      "metadata": {
        "id": "VHlCkKGX7W5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pymol import cmd\n",
        "\n",
        "# Set the input directory containing your .pdb files\n",
        "input_directory = '../top_pymol/simulation_0'\n",
        "# Set the output directory where you want to save the movie\n",
        "output_directory = '../movie_dir'\n",
        "# Define the output movie filename\n",
        "output_movie_filename = os.path.join(output_directory, 'simulation_movie.mp4')\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Clear existing objects in PyMOL\n",
        "cmd.reinitialize()\n",
        "\n",
        "# Load all .pdb files from the input directory\n",
        "pdb_files = sorted([f for f in os.listdir(input_directory) if f.endswith('.pdb')])[:50]\n",
        "\n",
        "\n",
        "# Create scenes for each frame\n",
        "for i, pdb_file in enumerate(pdb_files):\n",
        "\n",
        "    # Clear any previously loaded frames\n",
        "    cmd.delete('all')  # Remove all previous objects from the scene\n",
        "\n",
        "    # Load the current frame\n",
        "    cmd.load(os.path.join(input_directory, pdb_file), f'frame_{i}')\n",
        "\n",
        "\n",
        "    # Optionally set the view or other properties here\n",
        "    cmd.show('spheres', f'frame_{i}')  # Show as spheres (or other representation)\n",
        "\n",
        "    # You can customize colors or representations if needed\n",
        "    #cmd.color('blue', f'frame_{i}')  # Color all frames blue\n",
        "\n",
        "    # Create a new scene for this frame\n",
        "    cmd.scene(f'scene_{i}', 'store')  # 0 means current state\n",
        "\n",
        "    # Save the current scene as an image (PNG format)\n",
        "    image_filename = os.path.join(output_directory, f'frame_{i}.png')\n",
        "    cmd.png(image_filename, width=800, height=600, dpi=300)  # Adjust dimensions and DPI as needed\n",
        "\n",
        "# Invoke FFmpeg to create a video from the saved frames\n",
        "ffmpeg_command = f\"ffmpeg -framerate 1 -i {output_directory}/frame_%d.png -c:v libx264 -pix_fmt yuv420p {output_movie_filename}\"\n",
        "os.system(ffmpeg_command)\n",
        "\n",
        "print(f\"Movie saved to {output_movie_filename}\")"
      ],
      "metadata": {
        "id": "IwOckR2161ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup appropriate GNN model"
      ],
      "metadata": {
        "id": "vRaN5O97T_UN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Neighborhood calculation functions"
      ],
      "metadata": {
        "id": "2_fDJnFp1afV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import jax\n",
        "from jax_md import space, partition\n",
        "from jax_md.space import pairwise_displacement\n",
        "from jax import numpy as jnp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "class NeighborSearcher(object):\n",
        "    def __init__(self, box_size, cutoff):\n",
        "        # define a displacement function under periodic condition\n",
        "        self.box_size = jnp.array(box_size)\n",
        "\n",
        "        self.displacement_fn, _ = space.periodic(self.box_size)\n",
        "        self.disp = jax.vmap(self.displacement_fn)\n",
        "        self.dist = jax.vmap(space.metric(self.displacement_fn))\n",
        "        self.cutoff = cutoff\n",
        "        self.has_been_init = False\n",
        "        self.neighbor_list_fn = partition.neighbor_list(self.displacement_fn,\n",
        "                                                       self.box_size,\n",
        "                                                       cutoff,\n",
        "                                                       dr_threshold= cutoff / 6.,\n",
        "                                                       mask_self=False)\n",
        "        self.neighbor_list_fn_jit = jax.jit(self.neighbor_list_fn)\n",
        "        self.neighbor_dist_jit = self.displacement_fn\n",
        "\n",
        "    def init_new_neighbor_lst(self, pos):\n",
        "        # Create a new neighbor list.\n",
        "        pos = jnp.mod(pos, self.box_size)\n",
        "        nbr = self.neighbor_list_fn(pos)\n",
        "        self.has_been_init = True\n",
        "        return nbr\n",
        "\n",
        "    def update_neighbor_lst(self, pos, nbr):\n",
        "        pos = jnp.mod(pos, self.box_size)\n",
        "        # update_idx = np.any(self.dist(pos, nbr.reference_position) > (self.cutoff / 10.))\n",
        "\n",
        "        nbr = self.neighbor_list_fn_jit(pos, nbr)\n",
        "        if nbr.did_buffer_overflow:\n",
        "            nbr = self.neighbor_list_fn(pos)\n",
        "\n",
        "        return nbr\n",
        "\n",
        "\n",
        "def graph_network_nbr_fn(displacement_fn,\n",
        "                         cutoff,\n",
        "                         N):\n",
        "\n",
        "    def nbrlst_to_edge_mask(pos: jnp.ndarray, neigh_idx: jnp.ndarray):\n",
        "        # notice here, pos must be jax numpy array, otherwise fancy indexing will fail\n",
        "        d = partial(displacement_fn)\n",
        "        d = space.map_neighbor(d)\n",
        "        pos_neigh = pos[neigh_idx]\n",
        "        dR = d(pos, pos_neigh)\n",
        "\n",
        "        dr_2 = space.square_distance(dR)\n",
        "        mask = jnp.logical_and(neigh_idx != N, dr_2 < cutoff ** 2)\n",
        "\n",
        "        return mask\n",
        "\n",
        "    return nbrlst_to_edge_mask\n",
        "\n",
        "\n",
        "# def graph_network_nbr_fn_with_type_mask(displacement_fn,\n",
        "#                                          cutoff,\n",
        "#                                          N):\n",
        "#\n",
        "#     def nbrlst_to_edge_mask(pos: jnp.ndarray, neigh_idx: jnp.ndarray, type_mask: jnp.ndarray):\n",
        "#         # notice here, pos must be jax numpy array, otherwise fancy indexing will fail\n",
        "#         d = jax.partial(displacement_fn)\n",
        "#         d = space.map_neighbor(d)\n",
        "#         pos_neigh = pos[neigh_idx]\n",
        "#         dR = d(pos, pos_neigh)\n",
        "#\n",
        "#         dr_2 = space.square_distance(dR)\n",
        "#         mask = jnp.logical_and(neigh_idx != N, dr_2 < cutoff ** 2)\n",
        "#         mask = jnp.logical_and(type_mask, mask)\n",
        "#         return mask\n",
        "#\n",
        "#     return nbrlst_to_edge_mask\n"
      ],
      "metadata": {
        "id": "JQqn-NVF1Yah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GNN model definitions"
      ],
      "metadata": {
        "id": "RAIIfBTb1g5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.nn\n",
        "import dgl.function as fn\n",
        "from dgl.ops import edge_softmax\n",
        "from dgl.utils import expand_as_pair\n",
        "import time\n",
        "from md_module import get_neighbor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from typing import List, Set, Dict, Tuple, Optional\n",
        "\n",
        "\n",
        "def cubic_kernel(r, re):\n",
        "    eps = 1e-3\n",
        "    r = torch.threshold(r, eps, re)\n",
        "    return nn.ReLU()((1. - (r/re)**2)**3)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 hidden_dim=128,\n",
        "                 hidden_layer=3,\n",
        "                 activation_first=False,\n",
        "                 activation='relu',\n",
        "                 init_param=False):\n",
        "        super(MLP, self).__init__()\n",
        "        if activation == 'relu':\n",
        "            act_fn = nn.ReLU()\n",
        "        elif activation == 'leaky_relu':\n",
        "            act_fn = nn.LeakyReLU(0.2)\n",
        "        elif activation == 'sigmoid':\n",
        "            act_fn = nn.Sigmoid()\n",
        "        elif activation == 'tanh':\n",
        "            act_fn = nn.Tanh()\n",
        "        elif activation == 'elu':\n",
        "            act_fn = nn.ELU()\n",
        "        elif activation == 'gelu':\n",
        "            act_fn = nn.GELU()\n",
        "        elif activation == 'silu':\n",
        "            act_fn = nn.SiLU()\n",
        "        else:\n",
        "            raise Exception('Only support: relu, leaky_relu, sigmoid, tanh, elu, as non-linear activation')\n",
        "\n",
        "        mlp_layer = []\n",
        "        for l in range(hidden_layer):\n",
        "            if l != hidden_layer-1 and l != 0:\n",
        "                mlp_layer += [nn.Linear(hidden_dim, hidden_dim), act_fn]\n",
        "            elif l == 0:\n",
        "                if hidden_layer == 1:\n",
        "                    if activation_first:\n",
        "                        mlp_layer += [act_fn, nn.Linear(in_feats, out_feats)]\n",
        "                    else:\n",
        "                        print('Using MLP with no hidden layer and activations! Fall back to nn.Linear()')\n",
        "                        mlp_layer += [nn.Linear(in_feats, out_feats)]\n",
        "                elif not activation_first:\n",
        "                    mlp_layer += [nn.Linear(in_feats, hidden_dim), act_fn]\n",
        "                else:\n",
        "                    mlp_layer += [act_fn, nn.Linear(in_feats, hidden_dim), act_fn]\n",
        "            else:   # l == hidden_layer-1\n",
        "                mlp_layer += [nn.Linear(hidden_dim, out_feats)]\n",
        "        self.mlp_layer = nn.Sequential(*mlp_layer)\n",
        "        if init_param:\n",
        "            self._init_parameters()\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        for layer in self.mlp_layer:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, feat):\n",
        "        return self.mlp_layer(feat)\n",
        "\n",
        "\n",
        "class SmoothConvLayerNew(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_node_feats,\n",
        "                 in_edge_feats,\n",
        "                 out_node_feats,\n",
        "                 hidden_dim=128,\n",
        "                 activation='relu',\n",
        "                 drop_edge=True,\n",
        "                 update_edge_emb=False):\n",
        "\n",
        "        super(SmoothConvLayerNew, self).__init__()\n",
        "        self.drop_edge = drop_edge\n",
        "        self.update_edge_emb = update_edge_emb\n",
        "        if self.update_edge_emb:\n",
        "            self.edge_layer_norm = nn.LayerNorm(in_edge_feats)\n",
        "\n",
        "        # self.theta_src = nn.Linear(in_node_feats, hidden_dim)\n",
        "        self.edge_affine = MLP(in_edge_feats, hidden_dim, activation=activation, hidden_layer=2)\n",
        "        self.src_affine = nn.Linear(in_node_feats, hidden_dim)\n",
        "        self.dst_affine = nn.Linear(in_node_feats, hidden_dim)\n",
        "        self.theta_edge = MLP(hidden_dim, in_node_feats,\n",
        "                              hidden_dim=hidden_dim, activation=activation, activation_first=True,\n",
        "                              hidden_layer=2)\n",
        "        # self.theta = MLP(hidden_dim, hidden_dim, activation_first=True, hidden_layer=2)\n",
        "\n",
        "        self.phi_dst = nn.Linear(in_node_feats, hidden_dim)\n",
        "        self.phi_edge = nn.Linear(in_node_feats, hidden_dim)\n",
        "        self.phi = MLP(hidden_dim, out_node_feats,\n",
        "                       activation_first=True, hidden_layer=1, hidden_dim=hidden_dim, activation=activation)\n",
        "\n",
        "    def forward(self, g: dgl.DGLGraph, node_feat: torch.Tensor) -> torch.Tensor:\n",
        "        h = node_feat.clone()\n",
        "        with g.local_scope():\n",
        "            if self.drop_edge and self.training:\n",
        "                src_idx, dst_idx = g.edges()\n",
        "                e_feat = g.edata['e'].clone()\n",
        "                dropout_ratio = 0.2\n",
        "                idx = np.arange(dst_idx.shape[0])\n",
        "                np.random.shuffle(idx)\n",
        "                keep_idx = idx[:-int(idx.shape[0] * dropout_ratio)]\n",
        "                src_idx = src_idx[keep_idx]\n",
        "                dst_idx = dst_idx[keep_idx]\n",
        "                e_feat = e_feat[keep_idx]\n",
        "                g = dgl.graph((src_idx, dst_idx))\n",
        "                g.edata['e'] = e_feat\n",
        "            # for multi batch training\n",
        "            if g.is_block:\n",
        "                h_src = h\n",
        "                h_dst = h[:g.number_of_dst_nodes()]\n",
        "            else:\n",
        "                h_src = h_dst = h\n",
        "\n",
        "            g.srcdata['h'] = h_src\n",
        "            g.dstdata['h'] = h_dst\n",
        "            edge_idx = g.edges()\n",
        "            src_idx = edge_idx[0]\n",
        "            dst_idx = edge_idx[1]\n",
        "            edge_code = self.edge_affine(g.edata['e'])\n",
        "            src_code = self.src_affine(h_src[src_idx])\n",
        "            dst_code = self.dst_affine(h_dst[dst_idx])\n",
        "            g.edata['e_emb'] = self.theta_edge(edge_code+src_code+dst_code)\n",
        "            self.edge_message_neigh_center = src_code * g.edata['e_emb'] # Recording messages for messgage regularisation\n",
        "            self.input_node_embeddings = h # Recording messages for node embedding regularisation\n",
        "\n",
        "            if self.update_edge_emb:\n",
        "                normalized_e_emb = self.edge_layer_norm(g.edata['e_emb'])\n",
        "            g.update_all(fn.u_mul_e('h', 'e_emb', 'm'), fn.sum('m', 'h'))\n",
        "            edge_emb = g.ndata['h']\n",
        "\n",
        "        if self.update_edge_emb:\n",
        "            g.edata['e'] = normalized_e_emb\n",
        "        node_feat = self.phi(self.phi_dst(h) + self.phi_edge(edge_emb))\n",
        "        return node_feat\n",
        "\n",
        "\n",
        "class SmoothConvBlockNew(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_node_feats,\n",
        "                 out_node_feats,\n",
        "                 hidden_dim=128,\n",
        "                 conv_layer=3,\n",
        "                 edge_emb_dim=64,\n",
        "                 use_layer_norm=False,\n",
        "                 use_batch_norm=True,\n",
        "                 drop_edge=False,\n",
        "                 activation='relu',\n",
        "                 update_egde_emb=False,\n",
        "                 ):\n",
        "        super(SmoothConvBlockNew, self).__init__()\n",
        "        self.conv = nn.ModuleList()\n",
        "        self.edge_emb_dim = edge_emb_dim\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        self.drop_edge = drop_edge\n",
        "        if use_batch_norm == use_layer_norm and use_batch_norm:\n",
        "            raise Exception('Only one type of normalization at a time')\n",
        "        if use_layer_norm or use_batch_norm:\n",
        "            self.norm_layers = nn.ModuleList()\n",
        "\n",
        "        for layer in range(conv_layer):\n",
        "            if layer == 0:\n",
        "                self.conv.append(SmoothConvLayerNew(in_node_feats=in_node_feats,\n",
        "                                                 in_edge_feats=self.edge_emb_dim,\n",
        "                                                 out_node_feats=out_node_feats,\n",
        "                                                 hidden_dim=hidden_dim,\n",
        "                                                 activation=activation,\n",
        "                                                 drop_edge=drop_edge,\n",
        "                                                 update_edge_emb=update_egde_emb))\n",
        "            else:\n",
        "                self.conv.append(SmoothConvLayerNew(in_node_feats=out_node_feats,\n",
        "                                                 in_edge_feats=self.edge_emb_dim,\n",
        "                                                 out_node_feats=out_node_feats,\n",
        "                                                 hidden_dim=hidden_dim,\n",
        "                                                 activation=activation,\n",
        "                                                 drop_edge=drop_edge,\n",
        "                                                 update_edge_emb=update_egde_emb))\n",
        "            if use_layer_norm:\n",
        "                self.norm_layers.append(nn.LayerNorm(out_node_feats))\n",
        "            elif use_batch_norm:\n",
        "                self.norm_layers.append(nn.BatchNorm1d(out_node_feats))\n",
        "\n",
        "    def forward(self, h: torch.Tensor, graph: dgl.DGLGraph) -> torch.Tensor:\n",
        "\n",
        "        for l, conv_layer in enumerate(self.conv):\n",
        "            if self.use_layer_norm or self.use_batch_norm:\n",
        "                h = conv_layer.forward(graph, self.norm_layers[l](h)) + h\n",
        "            else:\n",
        "                h = conv_layer.forward(graph, h) + h\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "# code from DGL documents\n",
        "class RBFExpansion(nn.Module):\n",
        "    r\"\"\"Expand distances between nodes by radial basis functions.\n",
        "\n",
        "    .. math::\n",
        "        \\exp(- \\gamma * ||d - \\mu||^2)\n",
        "\n",
        "    where :math:`d` is the distance between two nodes and :math:`\\mu` helps centralizes\n",
        "    the distances. We use multiple centers evenly distributed in the range of\n",
        "    :math:`[\\text{low}, \\text{high}]` with the difference between two adjacent centers\n",
        "    being :math:`gap`.\n",
        "\n",
        "    The number of centers is decided by :math:`(\\text{high} - \\text{low}) / \\text{gap}`.\n",
        "    Choosing fewer centers corresponds to reducing the resolution of the filter.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    low : float\n",
        "        Smallest center. Default to 0.\n",
        "    high : float\n",
        "        Largest center. Default to 30.\n",
        "    gap : float\n",
        "        Difference between two adjacent centers. :math:`\\gamma` will be computed as the\n",
        "        reciprocal of gap. Default to 0.1.\n",
        "    \"\"\"\n",
        "    def __init__(self, low=0., high=30., gap=0.1):\n",
        "        super(RBFExpansion, self).__init__()\n",
        "\n",
        "        num_centers = int(np.ceil((high - low) / gap))\n",
        "        self.centers = np.linspace(low, high, num_centers)\n",
        "        self.centers = nn.Parameter(torch.tensor(self.centers).float(), requires_grad=False)\n",
        "        self.gamma = 1 / gap\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reinitialize model parameters.\"\"\"\n",
        "        device = self.centers.device\n",
        "        self.centers = nn.Parameter(\n",
        "            self.centers.clone().detach().float(), requires_grad=False).to(device)\n",
        "\n",
        "    def forward(self, edge_dists):\n",
        "        \"\"\"Expand distances.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        edge_dists : float32 tensor of shape (E, 1)\n",
        "            Distances between end nodes of edges, E for the number of edges.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float32 tensor of shape (E, len(self.centers))\n",
        "            Expanded distances.\n",
        "        \"\"\"\n",
        "        radial = edge_dists - self.centers\n",
        "        coef = - self.gamma\n",
        "        return torch.exp(coef * (radial ** 2))\n",
        "\n",
        "\n",
        "class WaterMDDynamicBoxNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 encoding_size,\n",
        "                 out_feats,\n",
        "                 bond=None,       #\n",
        "                 hidden_dim=128,\n",
        "                 conv_layer=4,\n",
        "                 edge_embedding_dim=128,\n",
        "                 dropout=0.1,\n",
        "                 drop_edge=True,\n",
        "                 use_layer_norm=False,\n",
        "                 update_edge=False,\n",
        "                 expand_edge=True):\n",
        "        super(WaterMDDynamicBoxNet, self).__init__()\n",
        "        self.graph_conv = SmoothConvBlockNew(in_node_feats=encoding_size,\n",
        "                                              out_node_feats=encoding_size,\n",
        "                                              hidden_dim=hidden_dim,\n",
        "                                              conv_layer=conv_layer,\n",
        "                                              edge_emb_dim=edge_embedding_dim,\n",
        "                                              use_layer_norm=use_layer_norm,\n",
        "                                              use_batch_norm=not use_layer_norm,\n",
        "                                              drop_edge=drop_edge,\n",
        "                                              activation='silu',\n",
        "                                              update_egde_emb=update_edge)\n",
        "\n",
        "        self.edge_emb_dim = edge_embedding_dim\n",
        "        self.expand_edge = expand_edge\n",
        "        if self.expand_edge:\n",
        "            self.edge_expand = RBFExpansion(high=1, gap=0.025)\n",
        "        self.edge_drop_out = nn.Dropout(dropout)\n",
        "        self.use_bond = not bond is None\n",
        "\n",
        "        self.length_mean = nn.Parameter(torch.tensor([0.]), requires_grad=False)\n",
        "        self.length_std = nn.Parameter(torch.tensor([1.]), requires_grad=False)\n",
        "        self.length_scaler = StandardScaler()\n",
        "\n",
        "        self.node_encoder = nn.Linear(in_feats, encoding_size)\n",
        "        if bond is not None:\n",
        "            if self.expand_edge:\n",
        "                self.edge_encoder = MLP(4 + 1 + len(self.edge_expand.centers), self.edge_emb_dim, hidden_dim=hidden_dim,\n",
        "                                        activation='gelu')\n",
        "            else:\n",
        "                self.edge_encoder = MLP(4 + 1, self.edge_emb_dim, hidden_dim=hidden_dim,\n",
        "                                        activation='gelu')\n",
        "            self.bond_graph = self.build_bond_graph(bond)\n",
        "        else:\n",
        "            if self.expand_edge:\n",
        "                self.edge_encoder = MLP(3 + 1 + len(self.edge_expand.centers), self.edge_emb_dim, hidden_dim=hidden_dim,\n",
        "                                        activation='gelu')\n",
        "            else:\n",
        "                self.edge_encoder = MLP(3 + 1, self.edge_emb_dim, hidden_dim=hidden_dim,\n",
        "                                        activation='gelu')\n",
        "        self.edge_layer_norm = nn.LayerNorm(self.edge_emb_dim)\n",
        "        self.graph_decoder = MLP(encoding_size, out_feats, hidden_layer=2, hidden_dim=hidden_dim, activation='gelu')\n",
        "\n",
        "    def calc_edge_feat(self, rel_pos_periodic, rel_pos_norm):\n",
        "\n",
        "        if self.training:\n",
        "            self.fit_length(rel_pos_norm)\n",
        "            self._update_length_stat(self.length_scaler.mean_, np.sqrt(self.length_scaler.var_))\n",
        "        rel_pos_periodic = -rel_pos_periodic / (rel_pos_norm + 1e-8)\n",
        "        rel_pos_norm = (rel_pos_norm - self.length_mean) / self.length_std\n",
        "        if self.expand_edge:\n",
        "            edge_feat = torch.cat((rel_pos_periodic,\n",
        "                                   rel_pos_norm,\n",
        "                                   self.edge_expand(rel_pos_norm)), dim=1)\n",
        "        else:\n",
        "            edge_feat = torch.cat((rel_pos_periodic,\n",
        "                                   rel_pos_norm), dim=1)\n",
        "        return edge_feat\n",
        "\n",
        "    def build_graph(self, fluid_pos, cutoff, box_size, self_loop=True):\n",
        "        if isinstance(box_size, torch.Tensor):\n",
        "            box_size = box_size.to(fluid_pos.device)\n",
        "        elif isinstance(box_size, np.ndarray):\n",
        "            box_size = torch.from_numpy(box_size).to(fluid_pos.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            edge_idx, distance, distance_norm, _ = get_neighbor(fluid_pos,\n",
        "                                                                cutoff, box_size)\n",
        "        center_idx = edge_idx[0, :]  # [edge_num, 1]\n",
        "        neigh_idx = edge_idx[1, :]\n",
        "        fluid_graph = dgl.graph((neigh_idx, center_idx))\n",
        "        fluid_edge_feat = self.calc_edge_feat(distance, distance_norm.view(-1, 1))\n",
        "        if not self.use_bond:\n",
        "            fluid_edge_emb = self.edge_layer_norm(self.edge_encoder(fluid_edge_feat)) # [edge_num, 64]\n",
        "            fluid_edge_emb = self.edge_drop_out(fluid_edge_emb)\n",
        "            fluid_graph.edata['e'] = fluid_edge_emb\n",
        "        else:\n",
        "            edge_type = self.bond_graph.has_edges_between(center_idx, neigh_idx)\n",
        "            fluid_edge_feat = torch.cat((fluid_edge_feat, edge_type.view(-1, 1)), dim=1)\n",
        "            fluid_edge_emb = self.edge_layer_norm(self.edge_encoder(fluid_edge_feat))  # [edge_num, 64]\n",
        "            fluid_edge_emb = self.edge_drop_out(fluid_edge_emb)\n",
        "            fluid_graph.edata['e'] = fluid_edge_emb\n",
        "\n",
        "        # add self loop for fluid particles\n",
        "        if self_loop:\n",
        "            fluid_graph.add_self_loop()\n",
        "        return fluid_graph\n",
        "\n",
        "    def build_graph_batches(self, pos_lst, box_size_lst, cutoff):\n",
        "        graph_lst = []\n",
        "        for pos, box_size in zip(pos_lst, box_size_lst):\n",
        "            graph = self.build_graph(pos, cutoff, box_size)\n",
        "            graph_lst += [graph]\n",
        "        batched_graph = dgl.batch(graph_lst)\n",
        "        return batched_graph\n",
        "\n",
        "    def build_bond_graph(self, bond):\n",
        "        if isinstance(bond, np.ndarray):\n",
        "            bond = torch.from_numpy(bond).cuda()\n",
        "        bond_graph = dgl.graph((bond[:, 0], bond[:, 1]))\n",
        "        bond_graph = dgl.add_reverse_edges(bond_graph)  # undirectional and symmetry\n",
        "        return bond_graph\n",
        "\n",
        "    def _update_length_stat(self, new_mean, new_std):\n",
        "        self.length_mean[0] = new_mean[0]\n",
        "        self.length_std[0] = new_std[0]\n",
        "\n",
        "    def fit_length(self, length):\n",
        "        if not isinstance(length, np.ndarray):\n",
        "            length = length.detach().cpu().numpy().reshape(-1,1)\n",
        "        self.length_scaler.partial_fit(length)\n",
        "\n",
        "    def forward(self,\n",
        "                fluid_pos_lst,  #   list of [N, 3]\n",
        "                x,  # node feature    # [b*N, 3]\n",
        "                box_size_lst,   #   list of scalar\n",
        "                cutoff          # a scalar\n",
        "                ):\n",
        "        # fluid_graph = self.build_graph(fluid_pos, cutoff, box_size)\n",
        "        if len(fluid_pos_lst) > 1:\n",
        "            fluid_graph = self.build_graph_batches(fluid_pos_lst, box_size_lst, cutoff)\n",
        "        else:\n",
        "            fluid_graph = self.build_graph(fluid_pos_lst[0], cutoff, box_size_lst[0])\n",
        "\n",
        "        x = self.node_encoder(x)\n",
        "        x = self.graph_conv(x, fluid_graph)\n",
        "\n",
        "        x = self.graph_decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class WaterMDNetNew(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 encoding_size,\n",
        "                 out_feats,\n",
        "                 box_size,   # can also be array\n",
        "                 bond=None,       #\n",
        "                 hidden_dim=128,\n",
        "                 conv_layer=4,\n",
        "                 edge_embedding_dim=128,\n",
        "                 dropout=0.1,\n",
        "                 drop_edge=True,\n",
        "                 use_layer_norm=False):\n",
        "        super(WaterMDNetNew, self).__init__()\n",
        "        self.graph_conv = SmoothConvBlockNew(in_node_feats=encoding_size,\n",
        "                                             out_node_feats=encoding_size,\n",
        "                                             hidden_dim=hidden_dim,\n",
        "                                             conv_layer=conv_layer,\n",
        "                                             edge_emb_dim=edge_embedding_dim,\n",
        "                                             use_layer_norm=use_layer_norm,\n",
        "                                             use_batch_norm=not use_layer_norm,\n",
        "                                             drop_edge=drop_edge,\n",
        "                                             activation='silu')\n",
        "\n",
        "        self.edge_emb_dim = edge_embedding_dim\n",
        "        self.edge_expand = RBFExpansion(high=1, gap=0.025)\n",
        "        self.edge_drop_out = nn.Dropout(dropout)\n",
        "        self.use_bond = not bond is None\n",
        "\n",
        "        self.length_mean = nn.Parameter(torch.tensor([0.]), requires_grad=False)\n",
        "        self.length_std = nn.Parameter(torch.tensor([1.]), requires_grad=False)\n",
        "        self.length_scaler = StandardScaler()\n",
        "\n",
        "        if isinstance(box_size, np.ndarray):\n",
        "            self.box_size = torch.from_numpy(box_size).float()\n",
        "        else:\n",
        "            self.box_size = box_size\n",
        "        self.box_size = self.box_size\n",
        "\n",
        "        self.node_encoder = nn.Linear(in_feats, encoding_size)\n",
        "        if bond is not None:\n",
        "            self.edge_encoder = MLP(4 + 1 + len(self.edge_expand.centers), self.edge_emb_dim, hidden_dim=hidden_dim,\n",
        "                                    activation='gelu')\n",
        "            self.use_bond = True\n",
        "            self.bond_graph = self.build_bond_graph(bond)\n",
        "        else:\n",
        "            self.edge_encoder = MLP(3 + 1 + len(self.edge_expand.centers), self.edge_emb_dim, hidden_dim=hidden_dim,\n",
        "                                    activation='gelu')\n",
        "            self.use_bond = False\n",
        "        self.edge_layer_norm = nn.LayerNorm(self.edge_emb_dim)\n",
        "        self.graph_decoder = MLP(encoding_size, out_feats, hidden_layer=2, hidden_dim=hidden_dim, activation='gelu')\n",
        "\n",
        "    def calc_edge_feat(self,\n",
        "                       src_idx: torch.Tensor,\n",
        "                       dst_idx: torch.Tensor,\n",
        "                       pos_src: torch.Tensor,\n",
        "                       pos_dst=None) -> torch.Tensor:\n",
        "        # this is the raw input feature\n",
        "\n",
        "        # to enhance computation performance, dont track their calculation on graph\n",
        "        if pos_dst is None:\n",
        "            pos_dst = pos_src\n",
        "\n",
        "        with torch.no_grad():\n",
        "            rel_pos = pos_dst[dst_idx.long()] - pos_src[src_idx.long()]\n",
        "            if isinstance(self.box_size, torch.Tensor):\n",
        "                rel_pos_periodic = torch.remainder(rel_pos + 0.5 * self.box_size.to(rel_pos.device),\n",
        "                                                   self.box_size.to(rel_pos.device)) - 0.5 * self.box_size.to(rel_pos.device)\n",
        "            else:\n",
        "                rel_pos_periodic = torch.remainder(rel_pos + 0.5 * self.box_size,\n",
        "                                                   self.box_size) - 0.5 * self.box_size\n",
        "\n",
        "            rel_pos_norm = rel_pos_periodic.norm(dim=1).view(-1, 1)  # [edge_num, 1]\n",
        "            rel_pos_periodic /= rel_pos_norm + 1e-8   # normalized\n",
        "\n",
        "        if self.training:\n",
        "            self.fit_length(rel_pos_norm)\n",
        "            self._update_length_stat(self.length_scaler.mean_, np.sqrt(self.length_scaler.var_))\n",
        "\n",
        "        rel_pos_norm = (rel_pos_norm - self.length_mean) / self.length_std\n",
        "        edge_feat = torch.cat((rel_pos_periodic,\n",
        "                               rel_pos_norm,\n",
        "                               self.edge_expand(rel_pos_norm)), dim=1)\n",
        "        return edge_feat\n",
        "\n",
        "    def build_graph(self,\n",
        "                    fluid_edge_idx: torch.Tensor,\n",
        "                    fluid_pos: torch.Tensor,\n",
        "                    self_loop=True) -> dgl.DGLGraph:\n",
        "\n",
        "        center_idx = fluid_edge_idx[0, :]  # [edge_num, 1]\n",
        "        neigh_idx = fluid_edge_idx[1, :]\n",
        "        fluid_graph = dgl.graph((neigh_idx, center_idx))\n",
        "        fluid_edge_feat = self.calc_edge_feat(center_idx, neigh_idx, fluid_pos)\n",
        "\n",
        "        if not self.use_bond:\n",
        "            fluid_edge_emb = self.edge_layer_norm(self.edge_encoder(fluid_edge_feat))  # [edge_num, 64]\n",
        "            fluid_edge_emb = self.edge_drop_out(fluid_edge_emb)\n",
        "            fluid_graph.edata['e'] = fluid_edge_emb\n",
        "        else:\n",
        "            edge_type = self.bond_graph.has_edges_between(center_idx, neigh_idx)\n",
        "            fluid_edge_feat = torch.cat((fluid_edge_feat, edge_type.view(-1, 1)), dim=1)\n",
        "            fluid_edge_emb = self.edge_layer_norm(self.edge_encoder(fluid_edge_feat))  # [edge_num, 64]\n",
        "            fluid_edge_emb = self.edge_drop_out(fluid_edge_emb)\n",
        "            fluid_graph.edata['e'] = fluid_edge_emb\n",
        "\n",
        "        # add self loop for fluid particles\n",
        "        if self_loop:\n",
        "            fluid_graph.add_self_loop()\n",
        "        return fluid_graph\n",
        "\n",
        "    def build_graph_batches(self, pos_lst, edge_idx_lst):\n",
        "        graph_lst = []\n",
        "        for pos, edge_idx in zip(pos_lst, edge_idx_lst):\n",
        "            graph = self.build_graph(edge_idx, pos)\n",
        "            graph_lst += [graph]\n",
        "        batched_graph = dgl.batch(graph_lst)\n",
        "        return batched_graph\n",
        "\n",
        "    def build_bond_graph(self, bond) -> dgl.DGLGraph:\n",
        "        if isinstance(bond, np.ndarray):\n",
        "            bond = torch.from_numpy(bond).cuda()\n",
        "        bond_graph = dgl.graph((bond[:, 0], bond[:, 1]))\n",
        "        bond_graph = dgl.add_reverse_edges(bond_graph)  # undirectional and symmetry\n",
        "        return bond_graph\n",
        "\n",
        "    def _update_length_stat(self, new_mean, new_std):\n",
        "        self.length_mean[0] = new_mean[0]\n",
        "        self.length_std[0] = new_std[0]\n",
        "\n",
        "    def fit_length(self, length):\n",
        "        if not isinstance(length, np.ndarray):\n",
        "            length = length.detach().cpu().numpy().reshape(-1, 1)\n",
        "        self.length_scaler.partial_fit(length)\n",
        "\n",
        "    def forward(self,\n",
        "                fluid_pos_lst: List[torch.Tensor],  # list of [N, 3]\n",
        "                x: torch.Tensor,  # node feature    # [b*N, 3]\n",
        "                fluid_edge_lst: List[torch.Tensor]\n",
        "                ) -> torch.Tensor:\n",
        "        if len(fluid_pos_lst) > 1:\n",
        "            fluid_graph = self.build_graph_batches(fluid_pos_lst, fluid_edge_lst)\n",
        "        else:\n",
        "            fluid_graph = self.build_graph(fluid_edge_lst[0], fluid_pos_lst[0])\n",
        "        x = self.node_encoder(x)\n",
        "        x = self.graph_conv(x, fluid_graph)\n",
        "\n",
        "        x = self.graph_decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleMDNetNew(nn.Module):  # no bond, no learnable node encoder\n",
        "    def __init__(self,\n",
        "                 encoding_size,\n",
        "                 out_feats,\n",
        "                 box_size,   # can also be array\n",
        "                 hidden_dim=128,\n",
        "                 conv_layer=4,\n",
        "                 edge_embedding_dim=128,\n",
        "                 dropout=0.1,\n",
        "                 drop_edge=True,\n",
        "                 use_layer_norm=False):\n",
        "        super(SimpleMDNetNew, self).__init__()\n",
        "        self.graph_conv = SmoothConvBlockNew(in_node_feats=encoding_size,\n",
        "                                             out_node_feats=encoding_size,\n",
        "                                             hidden_dim=hidden_dim,\n",
        "                                             conv_layer=conv_layer,\n",
        "                                             edge_emb_dim=edge_embedding_dim,\n",
        "                                             use_layer_norm=use_layer_norm,\n",
        "                                             use_batch_norm=not use_layer_norm,\n",
        "                                             drop_edge=drop_edge,\n",
        "                                             activation='silu')\n",
        "\n",
        "        self.edge_emb_dim = edge_embedding_dim\n",
        "        self.edge_expand = RBFExpansion(high=1, gap=0.025)\n",
        "        self.edge_drop_out = nn.Dropout(dropout)\n",
        "\n",
        "        self.length_mean = nn.Parameter(torch.tensor([0.]), requires_grad=False)\n",
        "        self.length_std = nn.Parameter(torch.tensor([1.]), requires_grad=False)\n",
        "        self.length_scaler = StandardScaler()\n",
        "\n",
        "        if isinstance(box_size, np.ndarray):\n",
        "            self.box_size = torch.from_numpy(box_size).float()\n",
        "        else:\n",
        "            self.box_size = box_size\n",
        "        self.box_size = self.box_size\n",
        "\n",
        "        self.node_emb = nn.Parameter(torch.randn((1, encoding_size)), requires_grad=True)\n",
        "\n",
        "        self.edge_encoder = MLP(3 + 1 + len(self.edge_expand.centers), self.edge_emb_dim, hidden_dim=hidden_dim,\n",
        "                                activation='gelu')\n",
        "        '''\n",
        "        self.edge_encoder = MLP(3 + 2 + 1 + len(self.edge_expand.centers), self.edge_emb_dim, hidden_dim=hidden_dim,\n",
        "                                activation='gelu')\n",
        "        '''\n",
        "        self.edge_layer_norm = nn.LayerNorm(self.edge_emb_dim)\n",
        "        self.graph_decoder = MLP(encoding_size, out_feats, hidden_layer=2, hidden_dim=hidden_dim, activation='gelu')\n",
        "\n",
        "    def calc_edge_feat(self,\n",
        "                       src_idx: torch.Tensor,\n",
        "                       dst_idx: torch.Tensor,\n",
        "                       pos_src: torch.Tensor,\n",
        "                       pos_dst=None) -> torch.Tensor:\n",
        "        # this is the raw input feature\n",
        "\n",
        "        # to enhance computation performance, dont track their calculation on graph\n",
        "        if pos_dst is None:\n",
        "            pos_dst = pos_src\n",
        "\n",
        "        with torch.no_grad():\n",
        "            rel_pos = pos_dst[dst_idx.long()] - pos_src[src_idx.long()]\n",
        "            if isinstance(self.box_size, torch.Tensor):\n",
        "                rel_pos_periodic = torch.remainder(rel_pos + 0.5 * self.box_size.to(rel_pos.device),\n",
        "                                                   self.box_size.to(rel_pos.device)) - 0.5 * self.box_size.to(rel_pos.device)\n",
        "            else:\n",
        "                rel_pos_periodic = torch.remainder(rel_pos + 0.5 * self.box_size,\n",
        "                                                   self.box_size) - 0.5 * self.box_size\n",
        "\n",
        "            rel_pos_norm = rel_pos_periodic.norm(dim=1).view(-1, 1)  # [edge_num, 1]\n",
        "            rel_pos_periodic /= rel_pos_norm + 1e-8   # normalized\n",
        "\n",
        "        if self.training:\n",
        "            self.fit_length(rel_pos_norm)\n",
        "            self._update_length_stat(self.length_scaler.mean_, np.sqrt(self.length_scaler.var_))\n",
        "\n",
        "        rel_pos_norm = (rel_pos_norm - self.length_mean) / self.length_std\n",
        "\n",
        "        '''\n",
        "        # Add inductive bias like r^-6, r^-12 to signal intramolecular potentials\n",
        "\n",
        "        r_neg_6_prior = rel_pos_norm.pow(-6)\n",
        "        # Replace inf values with 0\n",
        "        r_neg_6_prior[torch.isinf(r_neg_6_prior)] = 0.0\n",
        "        r_neg_12_prior = rel_pos_norm.pow(-12)\n",
        "        r_neg_12_prior[torch.isinf(r_neg_12_prior)] = 0.0\n",
        "\n",
        "\n",
        "        edge_feat = torch.cat((rel_pos_periodic,\n",
        "                               r_neg_6_prior,\n",
        "                               r_neg_12_prior,\n",
        "                               rel_pos_norm,\n",
        "                               self.edge_expand(rel_pos_norm)), dim=1)\n",
        "\n",
        "\n",
        "        # Min-Max Scaling to avoid Nans downstream\n",
        "        min_val = torch.min(edge_feat)\n",
        "        max_val = torch.max(edge_feat)\n",
        "        edge_feat = (edge_feat - min_val) / (max_val - min_val)\n",
        "\n",
        "        '''\n",
        "        edge_feat = torch.cat((rel_pos_periodic,\n",
        "                               rel_pos_norm,\n",
        "                               self.edge_expand(rel_pos_norm)), dim=1)\n",
        "\n",
        "        return edge_feat\n",
        "\n",
        "    def build_graph(self,\n",
        "                    fluid_edge_idx: torch.Tensor,\n",
        "                    fluid_pos: torch.Tensor,\n",
        "                    self_loop=True) -> dgl.DGLGraph:\n",
        "\n",
        "        center_idx = fluid_edge_idx[0, :]  # [edge_num, 1]\n",
        "        neigh_idx = fluid_edge_idx[1, :]\n",
        "        fluid_graph = dgl.graph((neigh_idx, center_idx))\n",
        "        fluid_edge_feat = self.calc_edge_feat(center_idx, neigh_idx, fluid_pos)\n",
        "\n",
        "        fluid_edge_emb = self.edge_layer_norm(self.edge_encoder(fluid_edge_feat))  # [edge_num, 64]\n",
        "        '''\n",
        "        if torch.isnan(fluid_edge_emb).any():\n",
        "          print(\"NANANANN---1\")\n",
        "          exit(0)\n",
        "        '''\n",
        "\n",
        "\n",
        "        fluid_edge_emb = self.edge_drop_out(fluid_edge_emb)\n",
        "        fluid_graph.edata['e'] = fluid_edge_emb\n",
        "\n",
        "        # add self loop for fluid particles\n",
        "        if self_loop:\n",
        "            fluid_graph.add_self_loop()\n",
        "        return fluid_graph\n",
        "\n",
        "    def build_graph_batches(self, pos_lst, edge_idx_lst):\n",
        "        graph_lst = []\n",
        "        for pos, edge_idx in zip(pos_lst, edge_idx_lst):\n",
        "            graph = self.build_graph(edge_idx, pos)\n",
        "            graph_lst += [graph]\n",
        "        batched_graph = dgl.batch(graph_lst)\n",
        "        return batched_graph\n",
        "\n",
        "    def _update_length_stat(self, new_mean, new_std):\n",
        "        self.length_mean[0] = new_mean[0]\n",
        "        self.length_std[0] = new_std[0]\n",
        "\n",
        "    def fit_length(self, length):\n",
        "        if not isinstance(length, np.ndarray):\n",
        "            length = length.detach().cpu().numpy().reshape(-1, 1)\n",
        "        self.length_scaler.partial_fit(length)\n",
        "\n",
        "    def forward(self,\n",
        "                fluid_pos_lst: List[torch.Tensor],  # list of [N, 3]\n",
        "                fluid_edge_lst: List[torch.Tensor]\n",
        "                ) -> torch.Tensor:\n",
        "        if len(fluid_pos_lst) > 1:\n",
        "            fluid_graph = self.build_graph_batches(fluid_pos_lst, fluid_edge_lst)\n",
        "        else:\n",
        "            fluid_graph = self.build_graph(fluid_edge_lst[0], fluid_pos_lst[0])\n",
        "        num = np.sum([pos.shape[0] for pos in fluid_pos_lst])\n",
        "        x = self.node_emb.repeat((num, 1))\n",
        "        x = self.graph_conv(x, fluid_graph)\n",
        "\n",
        "        x = self.graph_decoder(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FRUyo40Y1GSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train GNN and record edge messages"
      ],
      "metadata": {
        "id": "Q4AJx60kUGOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train for LJ system test-cases"
      ],
      "metadata": {
        "id": "5r6rNjwl1vTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os, sys\n",
        "import joblib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import cupy\n",
        "\n",
        "sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
        "from nn_module import SimpleMDNetNew\n",
        "from train_utils import LJDataNew\n",
        "from graph_utils import NeighborSearcher, graph_network_nbr_fn\n",
        "import time\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" # just to test if it works w/o gpu\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "# for water box\n",
        "#CUTOFF_RADIUS = 7.5\n",
        "#CUTOFF_RADIUS = 10.2\n",
        "CUTOFF_RADIUS = 15.2\n",
        "BOX_SIZE = 27.27\n",
        "\n",
        "NUM_OF_ATOMS = 258\n",
        "\n",
        "# NUM_OF_ATOMS = 251 * 3  # tip4p\n",
        "# CUTOFF_RADIUS = 3.4\n",
        "\n",
        "LAMBDA1 = 100.\n",
        "LAMBDA2 = 1e-3\n",
        "\n",
        "\n",
        "def get_rotation_matrix():\n",
        "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
        "        rotation is per shape based along up direction\n",
        "        Input:\n",
        "          Nx3 array, original point clouds\n",
        "        Return:\n",
        "          Nx3 array, rotated point clouds\n",
        "    \"\"\"\n",
        "    if np.random.uniform() < 0.3:\n",
        "        angles = np.random.randint(-2, 2, size=(3,)) * np.pi\n",
        "    else:\n",
        "        angles = [0., 0., 0.]\n",
        "    Rx = np.array([[1., 0, 0],\n",
        "                       [0, np.cos(angles[0]), -np.sin(angles[0])],\n",
        "                       [0, np.sin(angles[0]), np.cos(angles[0])]], dtype=np.float32)\n",
        "    Ry = np.array([[np.cos(angles[1]), 0, np.sin(angles[1])],\n",
        "                       [0, 1, 0],\n",
        "                       [-np.sin(angles[1]), 0, np.cos(angles[1])]], dtype=np.float32)\n",
        "    Rz = np.array([[np.cos(angles[2]), -np.sin(angles[2]), 0],\n",
        "                   [np.sin(angles[2]), np.cos(angles[2]), 0],\n",
        "                   [0, 0, 1]], dtype=np.float32)\n",
        "    rotation_matrix = np.matmul(Rz, np.matmul(Ry, Rx))\n",
        "\n",
        "    return rotation_matrix\n",
        "\n",
        "\n",
        "def center_positions(pos):\n",
        "    offset = np.mean(pos, axis=0)\n",
        "    return pos - offset, offset\n",
        "\n",
        "def build_model(args, ckpt=None):\n",
        "\n",
        "    param_dict = {\n",
        "                  'encoding_size': args.encoding_size,\n",
        "                  'out_feats': 3,\n",
        "                  'hidden_dim': args.hidden_dim,\n",
        "                  'edge_embedding_dim': args.edge_embedding_dim,\n",
        "                  'conv_layer': 4,\n",
        "                  'drop_edge': args.drop_edge,\n",
        "                  'use_layer_norm': args.use_layer_norm,\n",
        "                  'box_size': BOX_SIZE,\n",
        "                  }\n",
        "\n",
        "    print(\"Using following set of hyper-parameters\")\n",
        "    print(param_dict)\n",
        "    model = SimpleMDNetNew(**param_dict)\n",
        "\n",
        "    if ckpt is not None:\n",
        "        print('Loading model weights from: ', ckpt)\n",
        "        model.load_state_dict((torch.load(ckpt)))\n",
        "    return model\n",
        "\n",
        "\n",
        "class ParticleNetLightning(pl.LightningModule):\n",
        "    def __init__(self, args, num_device=1, epoch_num=100, batch_size=1, learning_rate=3e-4, log_freq=1000,\n",
        "                 model_weights_ckpt=None, scaler_ckpt=None):\n",
        "        super(ParticleNetLightning, self).__init__()\n",
        "        self.pnet_model = build_model(args, model_weights_ckpt)\n",
        "        self.epoch_num = epoch_num\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.num_device = num_device\n",
        "        self.log_freq = log_freq\n",
        "        self.train_data_scaler = StandardScaler()\n",
        "        self.training_mean = np.array([0.])\n",
        "        self.training_var = np.array([1.])\n",
        "\n",
        "        if scaler_ckpt is not None:\n",
        "            self.load_training_stats(scaler_ckpt)\n",
        "\n",
        "        self.cutoff = CUTOFF_RADIUS\n",
        "        self.nbr_searcher = NeighborSearcher(BOX_SIZE, self.cutoff)\n",
        "        self.nbrlst_to_edge_mask = jax.jit(graph_network_nbr_fn(self.nbr_searcher.displacement_fn,\n",
        "                                                                    self.cutoff,\n",
        "                                                                    NUM_OF_ATOMS))\n",
        "        self.nbr_cache = {}\n",
        "        self.rotate_aug = args.rotate_aug\n",
        "        self.data_dir = args.data_dir\n",
        "        self.loss_fn = args.loss\n",
        "        assert self.loss_fn in ['mae', 'mse', 'l1_message', 'kl_message', 'l1_message_node_embed', 'constrain_msg_stds']\n",
        "\n",
        "    def load_training_stats(self, scaler_ckpt):\n",
        "        if scaler_ckpt is not None:\n",
        "            scaler_info = np.load(scaler_ckpt)\n",
        "            self.training_mean = scaler_info['mean']\n",
        "            self.training_var = scaler_info['var']\n",
        "\n",
        "    def forward(self, pos, feat, edge_idx_tsr):\n",
        "        return self.denormalize(self.pnet_model(pos, feat, edge_idx_tsr.long()), self.training_var, self.training_mean)\n",
        "\n",
        "    def denormalize(self, normalized_force, var, mean):\n",
        "        return normalized_force * \\\n",
        "                np.sqrt(var) +\\\n",
        "                mean\n",
        "\n",
        "    def predict_forces(self, pos: np.ndarray, verbose=False):\n",
        "        nbr_start = time.time()\n",
        "        edge_idx_tsr = self.search_for_neighbor(pos,\n",
        "                                                self.nbr_searcher,\n",
        "                                                self.nbrlst_to_edge_mask,\n",
        "                                                'all')\n",
        "        nbr_end = time.time()\n",
        "        # enforce periodic boundary\n",
        "        pos = np.mod(pos, np.array(BOX_SIZE))\n",
        "        pos = torch.from_numpy(pos).float().cuda()\n",
        "        force_start = time.time()\n",
        "        pred = self.pnet_model([pos],\n",
        "                               [edge_idx_tsr],\n",
        "                               )\n",
        "        force_end = time.time()\n",
        "        if verbose:\n",
        "            print('=============================================')\n",
        "            print(f'Nbr search used time: {nbr_end - nbr_start}')\n",
        "            print(f'Force eval used time: {force_end - force_start}')\n",
        "\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "\n",
        "        pred = self.denormalize(pred, self.training_var, self.training_mean)\n",
        "\n",
        "        return pred\n",
        "\n",
        "    def scale_force(self, force, scaler):\n",
        "        b_pnum, dims = force.shape\n",
        "        force_flat = force.reshape((-1, 1))\n",
        "        scaler.partial_fit(force_flat)\n",
        "        force = torch.from_numpy(scaler.transform(force_flat)).float().view(b_pnum, dims)\n",
        "        return force\n",
        "\n",
        "    def get_edge_idx(self, nbrs, pos_jax, mask):\n",
        "        dummy_center_idx = nbrs.idx.copy()\n",
        "        #dummy_center_idx = jax.ops.index_update(dummy_center_idx, None,\n",
        "        #                                        jnp.arange(pos_jax.shape[0]).reshape(-1, 1))\n",
        "        dummy_center_idx = dummy_center_idx.at[:].set(jnp.arange(pos_jax.shape[0]).reshape(-1, 1))\n",
        "        center_idx = dummy_center_idx.reshape(-1)\n",
        "        center_idx_ = cupy.asarray(center_idx)\n",
        "        center_idx_tsr = torch.as_tensor(center_idx_, device='cuda')\n",
        "\n",
        "        neigh_idx = nbrs.idx.reshape(-1)\n",
        "\n",
        "        # cast jax device array to cupy array so that it can be transferred to torch\n",
        "        neigh_idx = cupy.asarray(neigh_idx)\n",
        "        mask = cupy.asarray(mask)\n",
        "        mask = torch.as_tensor(mask, device='cuda')\n",
        "        flat_mask = mask.view(-1)\n",
        "        neigh_idx_tsr = torch.as_tensor(neigh_idx, device='cuda')\n",
        "\n",
        "        edge_idx_tsr = torch.cat((center_idx_tsr[flat_mask].view(1, -1), neigh_idx_tsr[flat_mask].view(1, -1)),\n",
        "                                 dim=0)\n",
        "        return edge_idx_tsr\n",
        "\n",
        "    def search_for_neighbor(self, pos, nbr_searcher, masking_fn, type_name):\n",
        "        pos_jax = jax.device_put(pos, jax.devices(\"gpu\")[0])\n",
        "\n",
        "        if not nbr_searcher.has_been_init:\n",
        "            nbrs = nbr_searcher.init_new_neighbor_lst(pos_jax)\n",
        "            self.nbr_cache[type_name] = nbrs\n",
        "        else:\n",
        "            nbrs = nbr_searcher.update_neighbor_lst(pos_jax, self.nbr_cache[type_name])\n",
        "            self.nbr_cache[type_name] = nbrs\n",
        "\n",
        "        edge_mask_all = masking_fn(pos_jax, nbrs.idx)\n",
        "        edge_idx_tsr = self.get_edge_idx(nbrs, pos_jax, edge_mask_all)\n",
        "        return edge_idx_tsr.long()\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        pos_lst = batch['pos']\n",
        "        gt_lst = batch['forces']\n",
        "        edge_idx_lst = []\n",
        "        for b in range(len(gt_lst)):\n",
        "            pos, gt = pos_lst[b], gt_lst[b]\n",
        "\n",
        "            if self.rotate_aug:\n",
        "                pos = np.mod(pos, BOX_SIZE)\n",
        "                pos, off = center_positions(pos)\n",
        "                R = get_rotation_matrix()\n",
        "                pos = np.matmul(pos, R)\n",
        "                pos += off\n",
        "                gt = np.matmul(gt, R)\n",
        "\n",
        "            pos = np.mod(pos, BOX_SIZE)\n",
        "\n",
        "            gt = self.scale_force(gt, self.train_data_scaler).cuda()\n",
        "            pos_lst[b] = torch.from_numpy(pos).float().cuda()\n",
        "            gt_lst[b] = gt\n",
        "\n",
        "            edge_idx_tsr = self.search_for_neighbor(pos,\n",
        "                                                    self.nbr_searcher,\n",
        "                                                    self.nbrlst_to_edge_mask,\n",
        "                                                    'all')\n",
        "            edge_idx_lst += [edge_idx_tsr]\n",
        "        gt = torch.cat(gt_lst, dim=0)\n",
        "        pos_lst = [pos + torch.randn_like(pos) * 0.005 for pos in pos_lst]\n",
        "\n",
        "        pred = self.pnet_model(pos_lst,\n",
        "                               edge_idx_lst,\n",
        "                               )\n",
        "\n",
        "        if self.loss_fn == 'mae':\n",
        "            loss = nn.L1Loss()(pred, gt)\n",
        "        elif self.loss_fn == 'l1_message':\n",
        "            regularization = 1e-2\n",
        "            m12 = self.pnet_model.graph_conv.conv[-1].edge_message_neigh_center\n",
        "            normalized_l05 = torch.mean(torch.abs(m12))\n",
        "            mae = nn.L1Loss()(pred, gt)\n",
        "            message_regularization_term = regularization * normalized_l05\n",
        "            loss = mae + message_regularization_term\n",
        "        elif self.loss_fn == 'kl_message':\n",
        "            mae = nn.L1Loss()(pred, gt)\n",
        "            raw_msg = self.pnet_model.graph_conv.conv[-1].edge_message_neigh_center\n",
        "            mu = raw_msg[:, 0::2]\n",
        "            logvar = raw_msg[:, 1::2]\n",
        "            full_kl = torch.mean(torch.exp(logvar) + mu**2 - logvar)/2.0\n",
        "            loss = mae + full_kl\n",
        "        elif self.loss_fn == 'l1_message_node_embed':\n",
        "            regularization = 1e-1\n",
        "            m12 = self.pnet_model.graph_conv.conv[-1].edge_message_neigh_center\n",
        "            normalized_l05 = torch.mean(torch.abs(m12))\n",
        "            message_regularization_term = regularization * normalized_l05\n",
        "\n",
        "            n12 = self.pnet_model.graph_conv.conv[-1].input_node_embeddings\n",
        "            normalized_n05 = torch.mean(torch.abs(n12))\n",
        "            node_embed_regularization_term = regularization * normalized_n05\n",
        "\n",
        "            mae = nn.L1Loss()(pred, gt)\n",
        "\n",
        "            loss = mae + message_regularization_term + node_embed_regularization_term\n",
        "\n",
        "        elif self.loss_fn == 'constrain_msg_stds':\n",
        "            mae = nn.L1Loss()(pred, gt)\n",
        "            regularization = 1e-1\n",
        "            m12 = self.pnet_model.graph_conv.conv[-1].edge_message_neigh_center\n",
        "            std_remaining_abs = torch.abs(torch.std(m12[:, 1:], dim=0)) # k = 1, to align edge messages with pair-potential prediction\n",
        "            mean_std_remaining_abs = torch.mean(std_remaining_abs)\n",
        "            loss = mae + regularization * mean_std_remaining_abs # inductive bias to push all info to first k message components.\n",
        "        else:\n",
        "            loss = nn.MSELoss()(pred, gt)\n",
        "\n",
        "        #conservative_loss = (torch.mean(pred)).abs()\n",
        "        #loss = loss + LAMBDA2 * conservative_loss\n",
        "\n",
        "        self.training_mean = self.train_data_scaler.mean_\n",
        "        self.training_var = self.train_data_scaler.var_\n",
        "\n",
        "        self.log('total loss', loss, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log(f'{self.loss_fn} loss', loss, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log('var', torch.tensor(np.sqrt(self.training_var)), on_step=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        sched = StepLR(optim, step_size=5, gamma=0.001**(5/self.epoch_num))\n",
        "        return [optim], [sched]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        dataset = LJDataNew(dataset_path=os.path.join(self.data_dir, ''),\n",
        "                               sample_num=1000,\n",
        "                               case_prefix='data_',\n",
        "                               seed_num=10,\n",
        "                               mode='train')\n",
        "\n",
        "        return DataLoader(dataset, num_workers=2, batch_size=self.batch_size, shuffle=True,\n",
        "                          collate_fn=\n",
        "                          lambda batches: {\n",
        "                              'pos': [batch['pos'] for batch in batches],\n",
        "                              'forces': [batch['forces'] for batch in batches],\n",
        "                          })\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        dataset = LJDataNew(dataset_path=os.path.join(self.data_dir, ''),\n",
        "                               sample_num=1000,\n",
        "                               case_prefix='data_',\n",
        "                               seed_num=10,\n",
        "                               mode='test')\n",
        "\n",
        "        return DataLoader(dataset, num_workers=2, batch_size=16, shuffle=False,\n",
        "                          collate_fn=\n",
        "                          lambda batches: {\n",
        "                              'pos': [batch['pos'] for batch in batches],\n",
        "                              'forces': [batch['forces'] for batch in batches],\n",
        "                          })\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            pos_lst = batch['pos']\n",
        "            gt_lst = batch['forces']\n",
        "            edge_idx_lst = []\n",
        "            for b in range(len(gt_lst)):\n",
        "                pos, gt = pos_lst[b], gt_lst[b]\n",
        "                pos = np.mod(pos, BOX_SIZE)\n",
        "\n",
        "                gt = self.scale_force(gt, self.train_data_scaler).cuda()\n",
        "                pos_lst[b] = torch.from_numpy(pos).float().cuda()\n",
        "                gt_lst[b] = gt\n",
        "\n",
        "                edge_idx_tsr = self.search_for_neighbor(pos,\n",
        "                                                        self.nbr_searcher,\n",
        "                                                        self.nbrlst_to_edge_mask,\n",
        "                                                        'all')\n",
        "                edge_idx_lst += [edge_idx_tsr]\n",
        "            gt = torch.cat(gt_lst, dim=0)\n",
        "\n",
        "            pred = self.pnet_model(pos_lst,\n",
        "                                   edge_idx_lst,\n",
        "                                   )\n",
        "            ratio = torch.sqrt((pred.reshape(-1) - gt.reshape(-1)) ** 2) / (torch.abs(pred.reshape(-1)) + 1e-8)\n",
        "            outlier_ratio = ratio[ratio > 10.].shape[0] / ratio.shape[0]\n",
        "            mse = nn.MSELoss()(pred, gt)\n",
        "            mae = nn.L1Loss()(pred, gt)\n",
        "\n",
        "            batch_size = len(gt_lst)\n",
        "            self.log('val outlier', outlier_ratio, prog_bar=True, logger=True, batch_size = batch_size)\n",
        "            self.log('val mse', mse, prog_bar=True, logger=True, batch_size = batch_size)\n",
        "            self.log('val mae', mae, prog_bar=True, logger=True, batch_size = batch_size)\n",
        "\n",
        "\n",
        "class ModelCheckpointAtEpochEnd(pl.Callback):\n",
        "    \"\"\"\n",
        "       Save a checkpoint at epoch end\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            filepath,\n",
        "            save_step_frequency,\n",
        "            prefix=\"checkpoint\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            save_step_frequency: how often to save in steps\n",
        "            prefix: add a prefix to the name, only used if\n",
        "        \"\"\"\n",
        "        self.filepath = filepath\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_epoch_end(self, trainer: pl.Trainer, pl_module: ParticleNetLightning):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        if epoch % self.save_step_frequency == 0 or epoch == pl_module.epoch_num -1:\n",
        "            filename = os.path.join(self.filepath, f\"{self.prefix}_{epoch}.ckpt\")\n",
        "            scaler_filename = os.path.join(self.filepath, f\"scaler_{epoch}.npz\")\n",
        "\n",
        "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "            np.savez(scaler_filename,\n",
        "                     mean=pl_module.training_mean,\n",
        "                     var=pl_module.training_var,\n",
        "                     )\n",
        "            # joblib.dump(pl_module.train_data_scaler, scaler_filename)\n",
        "\n",
        "\n",
        "def train_model(args):\n",
        "    lr = args.lr\n",
        "    num_gpu = args.num_gpu\n",
        "    check_point_dir = args.cp_dir\n",
        "    min_epoch = args.min_epoch\n",
        "    max_epoch = args.max_epoch\n",
        "    weight_ckpt = args.state_ckpt_dir\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    model = ParticleNetLightning(epoch_num=max_epoch,\n",
        "                                 num_device=num_gpu if num_gpu != -1 else 1,\n",
        "                                 learning_rate=lr,\n",
        "                                 model_weights_ckpt=weight_ckpt,\n",
        "                                 batch_size=batch_size,\n",
        "                                 args=args)\n",
        "    cwd = os.getcwd()\n",
        "    model_check_point_dir = os.path.join(cwd, check_point_dir)\n",
        "    os.makedirs(model_check_point_dir, exist_ok=True)\n",
        "    #print(\"Checkpoints will be saved at: \", model_check_point_dir)\n",
        "    epoch_end_callback = ModelCheckpointAtEpochEnd(filepath=model_check_point_dir, save_step_frequency=1)\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
        "\n",
        "    trainer = Trainer(\n",
        "        devices=[0],#num_gpu,  # Use 'devices' instead of 'gpus'\n",
        "        accelerator='gpu',  # Specify the accelerator as 'gpu'\n",
        "        callbacks=[epoch_end_callback, checkpoint_callback],\n",
        "        min_epochs=min_epoch,\n",
        "        max_epochs=max_epoch,\n",
        "        precision=16,  # Use 'precision' for mixed precision if needed\n",
        "        benchmark=True,\n",
        "        strategy='ddp',  # Use 'strategy' for distributed training\n",
        "        default_root_dir='/home/pranav/gamd_sr/official/GAMD-main/code/LJ/model_ckpt'\n",
        "    )\n",
        "\n",
        "    # Get the version number used for this training run\n",
        "    version_number = trainer.logger.version\n",
        "    print(f\"Checkpoints are saved in: {model_check_point_dir}/lightning_logs/version_{version_number}/checkpoints/\")\n",
        "    trainer.fit(model)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--min_epoch', default=30, type=int)\n",
        "    parser.add_argument('--max_epoch', default=30, type=int)\n",
        "    parser.add_argument('--lr', default=3e-4, type=float)\n",
        "    parser.add_argument('--cp_dir', default='model_ckpt')\n",
        "    parser.add_argument('--state_ckpt_dir', default=None, type=str)\n",
        "    parser.add_argument('--batch_size', default=1, type=int)\n",
        "    parser.add_argument('--encoding_size', default=256, type=int)\n",
        "    parser.add_argument('--hidden_dim', default=128, type=int)\n",
        "    parser.add_argument('--edge_embedding_dim', default=256, type=int)\n",
        "    parser.add_argument('--drop_edge', action='store_true')\n",
        "    parser.add_argument('--use_layer_norm', action='store_true')\n",
        "    parser.add_argument('--disable_rotate_aug', dest='rotate_aug', default=True, action='store_false')\n",
        "    parser.add_argument('--data_dir', default='./md_dataset')\n",
        "    parser.add_argument('--loss', default='mae')\n",
        "    parser.add_argument('--num_gpu', default=-1, type=int)\n",
        "    args = parser.parse_args()\n",
        "    train_model(args)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "1lNk2zRu10FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python train_network_lj.py --num_gpu 1 --min_epoch 30 --max_epoch 30 --batch_size 1 --encoding_size 128 --hidden_dim 128 --edge_embedding_dim 128 --loss constrain_msg_stds --data_dir ../../../../top/  --use_layer_norm"
      ],
      "metadata": {
        "id": "2D7B9EQN13p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train for tip3p test-cases"
      ],
      "metadata": {
        "id": "8sv7oLzJ19hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import cupy\n",
        "\n",
        "sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
        "from nn_module import WaterMDNetNew\n",
        "from train_utils import WaterDataNew\n",
        "from graph_utils import NeighborSearcher, graph_network_nbr_fn\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # just to test if it works w/o gpu\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "# for water box\n",
        "CUTOFF_RADIUS = 4.2\n",
        "left_bound = 0.0\n",
        "right_bound = 20.0\n",
        "BOX_SIZE = right_bound - left_bound\n",
        "\n",
        "NUM_OF_ATOMS = 258 * 3\n",
        "\n",
        "# NUM_OF_ATOMS = 251 * 3  # tip4p\n",
        "# CUTOFF_RADIUS = 3.4\n",
        "\n",
        "LAMBDA1 = 100.\n",
        "LAMBDA2 = 1e-3\n",
        "\n",
        "\n",
        "def create_water_bond(total_atom_num):\n",
        "    bond = []\n",
        "    for i in range(0, total_atom_num, 3):\n",
        "        bond += [[i, i+1], [i, i+2]]\n",
        "    return np.array(bond)\n",
        "\n",
        "\n",
        "def get_rotation_matrix():\n",
        "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
        "        rotation is per shape based along up direction\n",
        "        Input:\n",
        "          Nx3 array, original point clouds\n",
        "        Return:\n",
        "          Nx3 array, rotated point clouds\n",
        "    \"\"\"\n",
        "    if np.random.uniform() < 0.3:\n",
        "        angles = np.random.randint(-2, 2, size=(3,)) * np.pi\n",
        "    else:\n",
        "        angles = [0., 0., 0.]\n",
        "    Rx = np.array([[1., 0, 0],\n",
        "                       [0, np.cos(angles[0]), -np.sin(angles[0])],\n",
        "                       [0, np.sin(angles[0]), np.cos(angles[0])]], dtype=np.float32)\n",
        "    Ry = np.array([[np.cos(angles[1]), 0, np.sin(angles[1])],\n",
        "                       [0, 1, 0],\n",
        "                       [-np.sin(angles[1]), 0, np.cos(angles[1])]], dtype=np.float32)\n",
        "    Rz = np.array([[np.cos(angles[2]), -np.sin(angles[2]), 0],\n",
        "                   [np.sin(angles[2]), np.cos(angles[2]), 0],\n",
        "                   [0, 0, 1]], dtype=np.float32)\n",
        "    rotation_matrix = np.matmul(Rz, np.matmul(Ry, Rx))\n",
        "\n",
        "    return rotation_matrix\n",
        "\n",
        "\n",
        "def center_positions(pos):\n",
        "    offset = np.mean(pos, axis=0)\n",
        "    return pos - offset, offset\n",
        "\n",
        "def build_model(args, ckpt=None):\n",
        "    bond_info = create_water_bond(NUM_OF_ATOMS)\n",
        "\n",
        "    param_dict = {'in_feats': 1,\n",
        "                  'encoding_size': args.encoding_size,\n",
        "                  'out_feats': 3,\n",
        "                  'bond': bond_info,\n",
        "                  'hidden_dim': args.hidden_dim,\n",
        "                  'edge_embedding_dim': args.edge_embedding_dim,\n",
        "                  'conv_layer': 4,\n",
        "                  'drop_edge': args.drop_edge,\n",
        "                  'use_layer_norm': args.use_layer_norm,\n",
        "                  'box_size': BOX_SIZE,\n",
        "                  }\n",
        "\n",
        "    print(\"Using following set of hyper-parameters\")\n",
        "    print(param_dict)\n",
        "    model = WaterMDNetNew(**param_dict)\n",
        "\n",
        "    if ckpt is not None:\n",
        "        print('Loading model weights from: ', ckpt)\n",
        "        model.load_state_dict((torch.load(ckpt)))\n",
        "    return model\n",
        "\n",
        "\n",
        "class ParticleNetLightning(pl.LightningModule):\n",
        "    def __init__(self, args, num_device=1, epoch_num=100, batch_size=1, learning_rate=3e-4, log_freq=1000,\n",
        "                 model_weights_ckpt=None, scaler_ckpt=None):\n",
        "        super(ParticleNetLightning, self).__init__()\n",
        "        self.pnet_model = build_model(args, model_weights_ckpt)\n",
        "        self.epoch_num = epoch_num\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.num_device = num_device\n",
        "        self.log_freq = log_freq\n",
        "        self.train_data_scaler = StandardScaler()\n",
        "        self.training_mean = np.array([0.])\n",
        "        self.training_var = np.array([1.])\n",
        "\n",
        "        if scaler_ckpt is not None:\n",
        "            self.load_training_stats(scaler_ckpt)\n",
        "\n",
        "        self.cutoff = CUTOFF_RADIUS\n",
        "        self.nbr_searcher = NeighborSearcher(BOX_SIZE, self.cutoff)\n",
        "        self.nbrlst_to_edge_mask = jax.jit(graph_network_nbr_fn(self.nbr_searcher.displacement_fn,\n",
        "                                                                    self.cutoff,\n",
        "                                                                    NUM_OF_ATOMS))\n",
        "        self.nbr_cache = {}\n",
        "        self.rotate_aug = args.rotate_aug\n",
        "        self.data_dir = args.data_dir\n",
        "        self.loss_fn = args.loss\n",
        "        assert self.loss_fn in ['mae', 'mse']\n",
        "\n",
        "    def load_training_stats(self, scaler_ckpt):\n",
        "        if scaler_ckpt is not None:\n",
        "            scaler_info = np.load(scaler_ckpt)\n",
        "            self.training_mean = scaler_info['mean']\n",
        "            self.training_var = scaler_info['var']\n",
        "\n",
        "    def forward(self, pos, feat, edge_idx_tsr):\n",
        "        return self.denormalize(self.pnet_model(pos, feat, edge_idx_tsr.long()), self.training_var, self.training_mean)\n",
        "\n",
        "    def denormalize(self, normalized_force, var, mean):\n",
        "        return normalized_force * \\\n",
        "                np.sqrt(var) +\\\n",
        "                mean\n",
        "\n",
        "    def predict_forces(self, feat: torch.Tensor, pos: np.ndarray):\n",
        "        edge_idx_tsr = self.search_for_neighbor(pos,\n",
        "                                                self.nbr_searcher,\n",
        "                                                self.nbrlst_to_edge_mask,\n",
        "                                                'all')\n",
        "        # enforce periodic boundary\n",
        "        pos = np.mod(pos, np.array(BOX_SIZE))\n",
        "        pos = torch.from_numpy(pos).float().to(feat.device)\n",
        "        pred = self.pnet_model([pos],\n",
        "                               feat,\n",
        "                               [edge_idx_tsr],\n",
        "                               )\n",
        "\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "\n",
        "        pred = self.denormalize(pred, self.training_var, self.training_mean)\n",
        "\n",
        "        return pred\n",
        "\n",
        "    def scale_force(self, force, scaler):\n",
        "        b_pnum, dims = force.shape\n",
        "        force_flat = force.reshape((-1, 1))\n",
        "        scaler.partial_fit(force_flat)\n",
        "        force = torch.from_numpy(scaler.transform(force_flat)).float().view(b_pnum, dims)\n",
        "        return force\n",
        "\n",
        "    def get_edge_idx(self, nbrs, pos_jax, mask):\n",
        "        dummy_center_idx = nbrs.idx.copy()\n",
        "        dummy_center_idx = jax.ops.index_update(dummy_center_idx, None,\n",
        "                                                jnp.arange(pos_jax.shape[0]).reshape(-1, 1))\n",
        "        center_idx = dummy_center_idx.reshape(-1)\n",
        "        center_idx_ = cupy.asarray(center_idx)\n",
        "        center_idx_tsr = torch.as_tensor(center_idx_, device='cuda')\n",
        "\n",
        "        neigh_idx = nbrs.idx.reshape(-1)\n",
        "\n",
        "        # cast jax device array to cupy array so that it can be transferred to torch\n",
        "        neigh_idx = cupy.asarray(neigh_idx)\n",
        "        mask = cupy.asarray(mask)\n",
        "        mask = torch.as_tensor(mask, device='cuda')\n",
        "        flat_mask = mask.view(-1)\n",
        "        neigh_idx_tsr = torch.as_tensor(neigh_idx, device='cuda')\n",
        "\n",
        "        edge_idx_tsr = torch.cat((center_idx_tsr[flat_mask].view(1, -1), neigh_idx_tsr[flat_mask].view(1, -1)),\n",
        "                                 dim=0)\n",
        "        return edge_idx_tsr\n",
        "\n",
        "    def search_for_neighbor(self, pos, nbr_searcher, masking_fn, type_name):\n",
        "        pos_jax = jax.device_put(pos, jax.devices(\"gpu\")[0])\n",
        "\n",
        "        if not nbr_searcher.has_been_init:\n",
        "            nbrs = nbr_searcher.init_new_neighbor_lst(pos_jax)\n",
        "            self.nbr_cache[type_name] = nbrs\n",
        "        else:\n",
        "            nbrs = nbr_searcher.update_neighbor_lst(pos_jax, self.nbr_cache[type_name])\n",
        "            self.nbr_cache[type_name] = nbrs\n",
        "\n",
        "        edge_mask_all = masking_fn(pos_jax, nbrs.idx)\n",
        "        edge_idx_tsr = self.get_edge_idx(nbrs, pos_jax, edge_mask_all)\n",
        "        return edge_idx_tsr.long()\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        feat, pos_lst = batch['feat'], batch['pos']\n",
        "        gt_lst = batch['forces']\n",
        "        edge_idx_lst = []\n",
        "        for b in range(len(gt_lst)):\n",
        "            pos, gt = pos_lst[b], gt_lst[b]\n",
        "\n",
        "            if self.rotate_aug:\n",
        "                pos = np.mod(pos, BOX_SIZE)\n",
        "                pos, off = center_positions(pos)\n",
        "                R = get_rotation_matrix()\n",
        "                pos = np.matmul(pos, R)\n",
        "                pos += off\n",
        "                gt = np.matmul(gt, R)\n",
        "\n",
        "            pos = np.mod(pos, BOX_SIZE)\n",
        "\n",
        "            gt = self.scale_force(gt, self.train_data_scaler).to(feat.device)\n",
        "            pos_lst[b] = torch.from_numpy(pos).float().to(feat.device)\n",
        "            gt_lst[b] = gt\n",
        "\n",
        "            edge_idx_tsr = self.search_for_neighbor(pos,\n",
        "                                                    self.nbr_searcher,\n",
        "                                                    self.nbrlst_to_edge_mask,\n",
        "                                                    'all')\n",
        "            edge_idx_lst += [edge_idx_tsr]\n",
        "        gt = torch.cat(gt_lst, dim=0)\n",
        "        pos_lst = [pos + torch.randn_like(pos) * 0.005 for pos in pos_lst]\n",
        "\n",
        "        pred = self.pnet_model(pos_lst,\n",
        "                               feat,\n",
        "                               edge_idx_lst,\n",
        "                               )\n",
        "\n",
        "        if self.loss_fn == 'mae':\n",
        "            loss = nn.L1Loss()(pred, gt)\n",
        "        else:\n",
        "            loss = nn.MSELoss()(pred, gt)\n",
        "\n",
        "        conservative_loss = (torch.mean(pred)).abs()\n",
        "        loss = loss + LAMBDA2 * conservative_loss\n",
        "\n",
        "        self.training_mean = self.train_data_scaler.mean_\n",
        "        self.training_var = self.train_data_scaler.var_\n",
        "\n",
        "        self.log('total loss', loss, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log(f'{self.loss_fn} loss', loss, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log('var', np.sqrt(self.training_var), on_step=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        sched = StepLR(optim, step_size=5, gamma=0.001**(5/self.epoch_num))\n",
        "        return [optim], [sched]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        dataset = WaterDataNew(dataset_path=os.path.join(self.data_dir, 'water_data_tip3p'),\n",
        "                               sample_num=1000,\n",
        "                               case_prefix='data_',\n",
        "                               seed_num=10,\n",
        "                               m_num=NUM_OF_ATOMS//3,\n",
        "                               mode='train',\n",
        "                               data_type='tip3p')\n",
        "\n",
        "        return DataLoader(dataset, num_workers=2, batch_size=self.batch_size, shuffle=True,\n",
        "                          collate_fn=\n",
        "                          lambda batches: {\n",
        "                              'feat': torch.cat([torch.from_numpy(batch['feat']).float() for batch in batches], dim=0),\n",
        "                              'pos': [batch['pos'] for batch in batches],\n",
        "                              'forces': [batch['forces'] for batch in batches],\n",
        "                          })\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        dataset = WaterDataNew(dataset_path=os.path.join(self.data_dir, 'water_data_tip3p'),\n",
        "                               sample_num=1000,\n",
        "                               case_prefix='data_',\n",
        "                               seed_num=10,\n",
        "                               m_num=NUM_OF_ATOMS//3,\n",
        "                               mode='test',\n",
        "                               data_type='tip3p')\n",
        "\n",
        "        return DataLoader(dataset, num_workers=2, batch_size=16, shuffle=False,\n",
        "                          collate_fn=\n",
        "                          lambda batches: {\n",
        "                              'feat': torch.cat([torch.from_numpy(batch['feat']).float() for batch in batches], dim=0),\n",
        "                              'pos': [batch['pos'] for batch in batches],\n",
        "                              'forces': [batch['forces'] for batch in batches],\n",
        "                          })\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            feat, pos_lst = batch['feat'], batch['pos']\n",
        "            gt_lst = batch['forces']\n",
        "            edge_idx_lst = []\n",
        "            for b in range(len(gt_lst)):\n",
        "                pos, gt = pos_lst[b], gt_lst[b]\n",
        "                pos = np.mod(pos, BOX_SIZE)\n",
        "\n",
        "                gt = self.scale_force(gt, self.train_data_scaler).to(feat.device)\n",
        "                pos_lst[b] = torch.from_numpy(pos).float().to(feat.device)\n",
        "                gt_lst[b] = gt\n",
        "\n",
        "                edge_idx_tsr = self.search_for_neighbor(pos,\n",
        "                                                        self.nbr_searcher,\n",
        "                                                        self.nbrlst_to_edge_mask,\n",
        "                                                        'all')\n",
        "                edge_idx_lst += [edge_idx_tsr]\n",
        "            gt = torch.cat(gt_lst, dim=0)\n",
        "\n",
        "            pred = self.pnet_model(pos_lst,\n",
        "                                   feat,\n",
        "                                   edge_idx_lst,\n",
        "                                   )\n",
        "            ratio = torch.sqrt((pred.reshape(-1) - gt.reshape(-1)) ** 2) / (torch.abs(pred.reshape(-1)) + 1e-8)\n",
        "            outlier_ratio = ratio[ratio > 10.].shape[0] / ratio.shape[0]\n",
        "            mse = nn.MSELoss()(pred, gt)\n",
        "            mae = nn.L1Loss()(pred, gt)\n",
        "\n",
        "            self.log('val outlier', outlier_ratio, prog_bar=True, logger=True)\n",
        "            self.log('val mse', mse, prog_bar=True, logger=True)\n",
        "            self.log('val mae', mae, prog_bar=True, logger=True)\n",
        "\n",
        "\n",
        "class ModelCheckpointAtEpochEnd(pl.Callback):\n",
        "    \"\"\"\n",
        "       Save a checkpoint at epoch end\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            filepath,\n",
        "            save_step_frequency,\n",
        "            prefix=\"checkpoint\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            save_step_frequency: how often to save in steps\n",
        "            prefix: add a prefix to the name, only used if\n",
        "        \"\"\"\n",
        "        self.filepath = filepath\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_epoch_end(self, trainer: pl.Trainer, pl_module: ParticleNetLightning):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        if epoch % self.save_step_frequency == 0 or epoch == pl_module.epoch_num -1:\n",
        "            filename = os.path.join(self.filepath, f\"{self.prefix}_{epoch}.ckpt\")\n",
        "            scaler_filename = os.path.join(self.filepath, f\"scaler_{epoch}.npz\")\n",
        "\n",
        "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "            np.savez(scaler_filename,\n",
        "                     mean=pl_module.training_mean,\n",
        "                     var=pl_module.training_var,\n",
        "                     )\n",
        "            # joblib.dump(pl_module.train_data_scaler, scaler_filename)\n",
        "\n",
        "\n",
        "def train_model(args):\n",
        "    lr = args.lr\n",
        "    num_gpu = args.num_gpu\n",
        "    check_point_dir = args.cp_dir\n",
        "    min_epoch = args.min_epoch\n",
        "    max_epoch = args.max_epoch\n",
        "    weight_ckpt = args.state_ckpt_dir\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    model = ParticleNetLightning(epoch_num=max_epoch,\n",
        "                                 num_device=num_gpu if num_gpu != -1 else 1,\n",
        "                                 learning_rate=lr,\n",
        "                                 model_weights_ckpt=weight_ckpt,\n",
        "                                 batch_size=batch_size,\n",
        "                                 args=args)\n",
        "    cwd = os.getcwd()\n",
        "    model_check_point_dir = os.path.join(cwd, check_point_dir)\n",
        "    os.makedirs(model_check_point_dir, exist_ok=True)\n",
        "    epoch_end_callback = ModelCheckpointAtEpochEnd(filepath=model_check_point_dir, save_step_frequency=5)\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
        "\n",
        "    trainer = Trainer(gpus=num_gpu,\n",
        "                      callbacks=[epoch_end_callback, checkpoint_callback],\n",
        "                      min_epochs=min_epoch,\n",
        "                      max_epochs=max_epoch,\n",
        "                      amp_backend='apex',\n",
        "                      amp_level='O1',\n",
        "                      benchmark=True,\n",
        "                      distributed_backend='ddp',\n",
        "                      )\n",
        "    trainer.fit(model)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--min_epoch', default=30, type=int)\n",
        "    parser.add_argument('--max_epoch', default=30, type=int)\n",
        "    parser.add_argument('--lr', default=3e-4, type=float)\n",
        "    parser.add_argument('--cp_dir', default='./model_ckpt')\n",
        "    parser.add_argument('--state_ckpt_dir', default=None, type=str)\n",
        "    parser.add_argument('--batch_size', default=1, type=int)\n",
        "    parser.add_argument('--encoding_size', default=256, type=int)\n",
        "    parser.add_argument('--hidden_dim', default=128, type=int)\n",
        "    parser.add_argument('--edge_embedding_dim', default=256, type=int)\n",
        "    parser.add_argument('--drop_edge', action='store_true')\n",
        "    parser.add_argument('--use_layer_norm', action='store_true')\n",
        "    parser.add_argument('--disable_rotate_aug', dest='rotate_aug', default=True, action='store_false')\n",
        "    parser.add_argument('--data_dir', default='./md_dataset')\n",
        "    parser.add_argument('--loss', default='mae')\n",
        "    parser.add_argument('--num_gpu', default=-1, type=int)\n",
        "    args = parser.parse_args()\n",
        "    train_model(args)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "ad8NFqiV2HXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train for tip4p test-cases"
      ],
      "metadata": {
        "id": "k6vG_wdl2JWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os, sys\n",
        "import joblib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import cupy\n",
        "\n",
        "sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
        "from nn_module import WaterMDNetNew\n",
        "from train_utils import WaterDataNew\n",
        "from graph_utils import NeighborSearcher, graph_network_nbr_fn\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # just to test if it works w/o gpu\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "# for water box\n",
        "CUTOFF_RADIUS = 4.2\n",
        "left_bound = 0.0\n",
        "right_bound = 20.0\n",
        "BOX_SIZE = right_bound - left_bound\n",
        "\n",
        "NUM_OF_ATOMS = 251 * 3  # 258 *3\n",
        "\n",
        "LAMBDA1 = 100.\n",
        "LAMBDA2 = 1e-3\n",
        "\n",
        "\n",
        "def create_water_bond(total_atom_num):\n",
        "    bond = []\n",
        "    for i in range(0, total_atom_num, 3):\n",
        "        bond += [[i, i+1], [i, i+2]]\n",
        "    return np.array(bond)\n",
        "\n",
        "\n",
        "def get_rotation_matrix():\n",
        "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
        "        rotation is per shape based along up direction\n",
        "        Input:\n",
        "          Nx3 array, original point clouds\n",
        "        Return:\n",
        "          Nx3 array, rotated point clouds\n",
        "    \"\"\"\n",
        "    if np.random.uniform() < 0.3:\n",
        "        angles = np.random.randint(-2, 2, size=(3,)) * np.pi\n",
        "    else:\n",
        "        angles = [0., 0., 0.]\n",
        "    Rx = np.array([[1., 0, 0],\n",
        "                       [0, np.cos(angles[0]), -np.sin(angles[0])],\n",
        "                       [0, np.sin(angles[0]), np.cos(angles[0])]], dtype=np.float32)\n",
        "    Ry = np.array([[np.cos(angles[1]), 0, np.sin(angles[1])],\n",
        "                       [0, 1, 0],\n",
        "                       [-np.sin(angles[1]), 0, np.cos(angles[1])]], dtype=np.float32)\n",
        "    Rz = np.array([[np.cos(angles[2]), -np.sin(angles[2]), 0],\n",
        "                   [np.sin(angles[2]), np.cos(angles[2]), 0],\n",
        "                   [0, 0, 1]], dtype=np.float32)\n",
        "    rotation_matrix = np.matmul(Rz, np.matmul(Ry, Rx))\n",
        "\n",
        "    return rotation_matrix\n",
        "\n",
        "\n",
        "def center_positions(pos):\n",
        "    offset = np.mean(pos, axis=0)\n",
        "    return pos - offset, offset\n",
        "\n",
        "def build_model(args, ckpt=None):\n",
        "    bond_info = create_water_bond(NUM_OF_ATOMS)\n",
        "\n",
        "    param_dict = {'in_feats': 1,\n",
        "                  'encoding_size': args.encoding_size,\n",
        "                  'out_feats': 3,\n",
        "                  'bond': bond_info,\n",
        "                  'hidden_dim': args.hidden_dim,\n",
        "                  'edge_embedding_dim': args.edge_embedding_dim,\n",
        "                  'conv_layer': 4,\n",
        "                  'drop_edge': args.drop_edge,\n",
        "                  'use_layer_norm': args.use_layer_norm,\n",
        "                  'box_size': BOX_SIZE,\n",
        "                  }\n",
        "\n",
        "    print(\"Using following set of hyper-parameters\")\n",
        "    print(param_dict)\n",
        "    model = WaterMDNetNew (**param_dict)\n",
        "\n",
        "    if ckpt is not None:\n",
        "        print('Loading model weights from: ', ckpt)\n",
        "        model.load_state_dict((torch.load(ckpt)))\n",
        "    return model\n",
        "\n",
        "\n",
        "class ParticleNetLightning(pl.LightningModule):\n",
        "    def __init__(self, args, num_device=1, epoch_num=100, batch_size=1, learning_rate=3e-4, log_freq=1000,\n",
        "                 model_weights_ckpt=None, scaler_ckpt=None):\n",
        "        super(ParticleNetLightning, self).__init__()\n",
        "        self.pnet_model = build_model(args, model_weights_ckpt)\n",
        "        self.epoch_num = epoch_num\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.num_device = num_device\n",
        "        self.log_freq = log_freq\n",
        "        self.train_data_scaler = StandardScaler()\n",
        "        self.training_mean = np.array([0.])\n",
        "        self.training_var = np.array([1.])\n",
        "\n",
        "        if scaler_ckpt is not None:\n",
        "            self.load_training_stats(scaler_ckpt)\n",
        "\n",
        "        self.cutoff = CUTOFF_RADIUS\n",
        "        self.nbr_searcher = NeighborSearcher(BOX_SIZE, self.cutoff)\n",
        "        self.nbrlst_to_edge_mask = jax.jit(graph_network_nbr_fn(self.nbr_searcher.displacement_fn,\n",
        "                                                                    self.cutoff,\n",
        "                                                                    NUM_OF_ATOMS))\n",
        "        self.nbr_cache = {}\n",
        "        self.rotate_aug = args.rotate_aug\n",
        "        self.data_dir = args.data_dir\n",
        "        self.loss_fn = args.loss\n",
        "        assert self.loss_fn in ['mae', 'mse']\n",
        "\n",
        "    def load_training_stats(self, scaler_ckpt):\n",
        "        if scaler_ckpt is not None:\n",
        "            scaler_info = np.load(scaler_ckpt)\n",
        "            self.training_mean = scaler_info['mean']\n",
        "            self.training_var = scaler_info['var']\n",
        "\n",
        "    def forward(self, pos, feat, edge_idx_tsr):\n",
        "        return self.denormalize(self.pnet_model(pos, feat, edge_idx_tsr.long()), self.training_var, self.training_mean)\n",
        "\n",
        "    def denormalize(self, normalized_force, var, mean):\n",
        "        return normalized_force * \\\n",
        "                np.sqrt(var) +\\\n",
        "                mean\n",
        "\n",
        "    def predict_forces(self, feat: torch.Tensor, pos: np.ndarray):\n",
        "        edge_idx_tsr = self.search_for_neighbor(pos,\n",
        "                                                self.nbr_searcher,\n",
        "                                                self.nbrlst_to_edge_mask,\n",
        "                                                'all')\n",
        "        # enforce periodic boundary\n",
        "        pos = np.mod(pos, np.array(BOX_SIZE))\n",
        "        pos = torch.from_numpy(pos).float().to(feat.device)\n",
        "        pred = self.pnet_model([pos],\n",
        "                               feat,\n",
        "                               [edge_idx_tsr],\n",
        "                               )\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "\n",
        "        pred = self.denormalize(pred, self.training_var, self.training_mean)\n",
        "\n",
        "        return pred\n",
        "\n",
        "    def scale_force(self, force, scaler):\n",
        "        b_pnum, dims = force.shape\n",
        "        force_flat = force.reshape((-1, 1))\n",
        "        scaler.partial_fit(force_flat)\n",
        "        force = torch.from_numpy(scaler.transform(force_flat)).float().view(b_pnum, dims)\n",
        "        return force\n",
        "\n",
        "    def get_edge_idx(self, nbrs, pos_jax, mask):\n",
        "        dummy_center_idx = nbrs.idx.copy()\n",
        "        dummy_center_idx = jax.ops.index_update(dummy_center_idx, None,\n",
        "                                                jnp.arange(pos_jax.shape[0]).reshape(-1, 1))\n",
        "        center_idx = dummy_center_idx.reshape(-1)\n",
        "        center_idx_ = cupy.asarray(center_idx)\n",
        "        center_idx_tsr = torch.as_tensor(center_idx_, device='cuda')\n",
        "\n",
        "        neigh_idx = nbrs.idx.reshape(-1)\n",
        "\n",
        "        # cast jax device array to cupy array so that it can be transferred to torch\n",
        "        neigh_idx = cupy.asarray(neigh_idx)\n",
        "        mask = cupy.asarray(mask)\n",
        "        mask = torch.as_tensor(mask, device='cuda')\n",
        "        flat_mask = mask.view(-1)\n",
        "        neigh_idx_tsr = torch.as_tensor(neigh_idx, device='cuda')\n",
        "\n",
        "        edge_idx_tsr = torch.cat((center_idx_tsr[flat_mask].view(1, -1), neigh_idx_tsr[flat_mask].view(1, -1)),\n",
        "                                 dim=0)\n",
        "        return edge_idx_tsr\n",
        "\n",
        "    def search_for_neighbor(self, pos, nbr_searcher, masking_fn, type_name):\n",
        "        pos_jax = jax.device_put(pos, jax.devices(\"gpu\")[0])\n",
        "\n",
        "        if not nbr_searcher.has_been_init:\n",
        "            nbrs = nbr_searcher.init_new_neighbor_lst(pos_jax)\n",
        "            self.nbr_cache[type_name] = nbrs\n",
        "        else:\n",
        "            nbrs = nbr_searcher.update_neighbor_lst(pos_jax, self.nbr_cache[type_name])\n",
        "            self.nbr_cache[type_name] = nbrs\n",
        "\n",
        "        edge_mask_all = masking_fn(pos_jax, nbrs.idx)\n",
        "        edge_idx_tsr = self.get_edge_idx(nbrs, pos_jax, edge_mask_all)\n",
        "        return edge_idx_tsr.long()\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        feat, pos_lst = batch['feat'], batch['pos']\n",
        "        gt_lst = batch['forces']\n",
        "        edge_idx_lst = []\n",
        "        for b in range(len(gt_lst)):\n",
        "            pos, gt = pos_lst[b], gt_lst[b]\n",
        "\n",
        "            if self.rotate_aug:\n",
        "                pos = np.mod(pos, BOX_SIZE)\n",
        "                pos, off = center_positions(pos)\n",
        "                R = get_rotation_matrix()\n",
        "                pos = np.matmul(pos, R)\n",
        "                pos += off\n",
        "                gt = np.matmul(gt, R)\n",
        "\n",
        "            pos = np.mod(pos, BOX_SIZE)\n",
        "\n",
        "            gt = self.scale_force(gt, self.train_data_scaler).to(feat.device)\n",
        "            pos_lst[b] = torch.from_numpy(pos).float().to(feat.device)\n",
        "            gt_lst[b] = gt\n",
        "\n",
        "            edge_idx_tsr = self.search_for_neighbor(pos,\n",
        "                                                    self.nbr_searcher,\n",
        "                                                    self.nbrlst_to_edge_mask,\n",
        "                                                    'all')\n",
        "            edge_idx_lst += [edge_idx_tsr]\n",
        "        gt = torch.cat(gt_lst, dim=0)\n",
        "        pos_lst = [pos + torch.randn_like(pos) * 0.005 for pos in pos_lst]\n",
        "\n",
        "        pred = self.pnet_model(pos_lst,\n",
        "                               feat,\n",
        "                               edge_idx_lst,\n",
        "                               )\n",
        "\n",
        "        if self.loss_fn == 'mae':\n",
        "            loss = nn.L1Loss()(pred, gt)\n",
        "        else:\n",
        "            loss = nn.MSELoss()(pred, gt)\n",
        "\n",
        "        conservative_loss = (torch.mean(pred)).abs()\n",
        "        loss = loss + LAMBDA2 * conservative_loss\n",
        "\n",
        "        self.training_mean = self.train_data_scaler.mean_\n",
        "        self.training_var = self.train_data_scaler.var_\n",
        "\n",
        "        self.log('total loss', loss, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log(f'{self.loss_fn} loss', loss, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log('var', np.sqrt(self.training_var), on_step=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        sched = StepLR(optim, step_size=5, gamma=0.001**(5/self.epoch_num))\n",
        "        return [optim], [sched]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        dataset = WaterDataNew(dataset_path=os.path.join(self.data_dir, 'water_data_tip4p'),\n",
        "                               sample_num=1000,\n",
        "                               case_prefix='data_',\n",
        "                               seed_num=10,\n",
        "                               m_num=NUM_OF_ATOMS//3,\n",
        "                               mode='train',\n",
        "                               data_type='tip4p')\n",
        "\n",
        "        return DataLoader(dataset, num_workers=2, batch_size=self.batch_size, shuffle=True,\n",
        "                          collate_fn=\n",
        "                          lambda batches: {\n",
        "                              'feat': torch.cat([torch.from_numpy(batch['feat']).float() for batch in batches], dim=0),\n",
        "                              'pos': [batch['pos'] for batch in batches],\n",
        "                              'forces': [batch['forces'] for batch in batches],\n",
        "                          })\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        dataset = WaterDataNew(dataset_path=os.path.join(self.data_dir, 'water_data_tip4p'),\n",
        "                               sample_num=1000,\n",
        "                               case_prefix='data_',\n",
        "                               seed_num=10,\n",
        "                               m_num=NUM_OF_ATOMS//3,\n",
        "                               mode='test',\n",
        "                               data_type='tip4p')\n",
        "\n",
        "        return DataLoader(dataset, num_workers=2, batch_size=16, shuffle=False,\n",
        "                          collate_fn=\n",
        "                          lambda batches: {\n",
        "                              'feat': torch.cat([torch.from_numpy(batch['feat']).float() for batch in batches], dim=0),\n",
        "                              'pos': [batch['pos'] for batch in batches],\n",
        "                              'forces': [batch['forces'] for batch in batches],\n",
        "                          })\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            feat, pos_lst = batch['feat'], batch['pos']\n",
        "            gt_lst = batch['forces']\n",
        "            edge_idx_lst = []\n",
        "            for b in range(len(gt_lst)):\n",
        "                pos, gt = pos_lst[b], gt_lst[b]\n",
        "                pos = np.mod(pos, BOX_SIZE)\n",
        "\n",
        "                gt = self.scale_force(gt, self.train_data_scaler).to(feat.device)\n",
        "                pos_lst[b] = torch.from_numpy(pos).float().to(feat.device)\n",
        "                gt_lst[b] = gt\n",
        "\n",
        "                edge_idx_tsr = self.search_for_neighbor(pos,\n",
        "                                                        self.nbr_searcher,\n",
        "                                                        self.nbrlst_to_edge_mask,\n",
        "                                                        'all')\n",
        "                edge_idx_lst += [edge_idx_tsr]\n",
        "            gt = torch.cat(gt_lst, dim=0)\n",
        "\n",
        "            pred = self.pnet_model(pos_lst,\n",
        "                                   feat,\n",
        "                                   edge_idx_lst,\n",
        "                                   )\n",
        "            ratio = torch.sqrt((pred.reshape(-1) - gt.reshape(-1)) ** 2) / (torch.abs(pred.reshape(-1)) + 1e-8)\n",
        "            outlier_ratio = ratio[ratio > 10.].shape[0] / ratio.shape[0]\n",
        "            mse = nn.MSELoss()(pred, gt)\n",
        "            mae = nn.L1Loss()(pred, gt)\n",
        "            self.log('val outlier', outlier_ratio, prog_bar=True, logger=True)\n",
        "            self.log('val mse', mse, prog_bar=True, logger=True)\n",
        "            self.log('val mae', mae, prog_bar=True, logger=True)\n",
        "\n",
        "\n",
        "class ModelCheckpointAtEpochEnd(pl.Callback):\n",
        "    \"\"\"\n",
        "       Save a checkpoint at epoch end\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            filepath,\n",
        "            save_step_frequency,\n",
        "            prefix=\"checkpoint\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            save_step_frequency: how often to save in steps\n",
        "            prefix: add a prefix to the name, only used if\n",
        "        \"\"\"\n",
        "        self.filepath = filepath\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_epoch_end(self, trainer: pl.Trainer, pl_module: ParticleNetLightning):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        if epoch % self.save_step_frequency == 0 or epoch == pl_module.epoch_num -1:\n",
        "            filename = os.path.join(self.filepath, f\"{self.prefix}_{epoch}.ckpt\")\n",
        "            scaler_filename = os.path.join(self.filepath, f\"scaler_{epoch}.npz\")\n",
        "\n",
        "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "            np.savez(scaler_filename,\n",
        "                     mean=pl_module.training_mean,\n",
        "                     var=pl_module.training_var,\n",
        "                     )\n",
        "            # joblib.dump(pl_module.train_data_scaler, scaler_filename)\n",
        "\n",
        "\n",
        "def train_model(args):\n",
        "    lr = args.lr\n",
        "    num_gpu = args.num_gpu\n",
        "    check_point_dir = args.cp_dir\n",
        "    min_epoch = args.min_epoch\n",
        "    max_epoch = args.max_epoch\n",
        "    weight_ckpt = args.state_ckpt_dir\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    model = ParticleNetLightning(epoch_num=max_epoch,\n",
        "                                 num_device=num_gpu if num_gpu != -1 else 1,\n",
        "                                 learning_rate=lr,\n",
        "                                 model_weights_ckpt=weight_ckpt,\n",
        "                                 batch_size=batch_size,\n",
        "                                 args=args)\n",
        "    cwd = os.getcwd()\n",
        "    model_check_point_dir = os.path.join(cwd, check_point_dir)\n",
        "    os.makedirs(model_check_point_dir, exist_ok=True)\n",
        "    epoch_end_callback = ModelCheckpointAtEpochEnd(filepath=model_check_point_dir, save_step_frequency=5)\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
        "\n",
        "    trainer = Trainer(gpus=num_gpu,\n",
        "                      callbacks=[epoch_end_callback, checkpoint_callback],\n",
        "                      min_epochs=min_epoch,\n",
        "                      max_epochs=max_epoch,\n",
        "                      amp_backend='apex',\n",
        "                      amp_level='O1',\n",
        "                      benchmark=True,\n",
        "                      distributed_backend='ddp',\n",
        "                      )\n",
        "    trainer.fit(model)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--min_epoch', default=30, type=int)\n",
        "    parser.add_argument('--max_epoch', default=30, type=int)\n",
        "    parser.add_argument('--lr', default=3e-4, type=float)\n",
        "    parser.add_argument('--cp_dir', default='./model_ckpt')\n",
        "    parser.add_argument('--state_ckpt_dir', default=None, type=str)\n",
        "    parser.add_argument('--batch_size', default=1, type=int)\n",
        "    parser.add_argument('--encoding_size', default=256, type=int)\n",
        "    parser.add_argument('--hidden_dim', default=128, type=int)\n",
        "    parser.add_argument('--edge_embedding_dim', default=256, type=int)\n",
        "    parser.add_argument('--drop_edge', action='store_true')\n",
        "    parser.add_argument('--use_layer_norm', action='store_true')\n",
        "    parser.add_argument('--disable_rotate_aug', dest='rotate_aug', default=True, action='store_false')\n",
        "    parser.add_argument('--data_dir', default='./md_dataset')\n",
        "    parser.add_argument('--loss', default='mae')\n",
        "    parser.add_argument('--num_gpu', default=-1, type=int)\n",
        "    args = parser.parse_args()\n",
        "    train_model(args)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "x10PDmsy2NRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train for DFT test-cases"
      ],
      "metadata": {
        "id": "ffv5qgmw2R0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import cupy\n",
        "\n",
        "sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
        "from nn_module import WaterMDDynamicBoxNet\n",
        "from train_utils import WaterDataRealLarge\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # just to test if it works w/o gpu\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "# for water box\n",
        "# CUTOFF_RADIUS = 9.5\n",
        "# left_bound = 0.0\n",
        "# right_bound = 12.4172\n",
        "# BOX_SIZE = right_bound - left_bound\n",
        "# NUM_OF_ATOMS = 258 * 3\n",
        "\n",
        "# CUTOFF_RADIUS = 3.4\n",
        "\n",
        "LAMBDA1 = 100.\n",
        "LAMBDA2 = 0.5e-2\n",
        "\n",
        "\n",
        "def create_water_bond(total_atom_num):\n",
        "    bond = []\n",
        "    for i in range(0, total_atom_num, 3):\n",
        "        bond += [[i, i+1], [i, i+2]]\n",
        "    return np.array(bond)\n",
        "\n",
        "\n",
        "def get_rotation_matrix():\n",
        "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
        "        rotation is per shape based along up direction\n",
        "        Input:\n",
        "          Nx3 array, original point clouds\n",
        "        Return:\n",
        "          Nx3 array, rotated point clouds\n",
        "    \"\"\"\n",
        "    if np.random.uniform() < 0.3:\n",
        "        angles = np.random.randint(-2, 2, size=(3,)) * np.pi\n",
        "    else:\n",
        "        angles = [0., 0., 0.]\n",
        "    Rx = np.array([[1., 0, 0],\n",
        "                       [0, np.cos(angles[0]), -np.sin(angles[0])],\n",
        "                       [0, np.sin(angles[0]), np.cos(angles[0])]], dtype=np.float32)\n",
        "    Ry = np.array([[np.cos(angles[1]), 0, np.sin(angles[1])],\n",
        "                       [0, 1, 0],\n",
        "                       [-np.sin(angles[1]), 0, np.cos(angles[1])]], dtype=np.float32)\n",
        "    Rz = np.array([[np.cos(angles[2]), -np.sin(angles[2]), 0],\n",
        "                   [np.sin(angles[2]), np.cos(angles[2]), 0],\n",
        "                   [0, 0, 1]], dtype=np.float32)\n",
        "    rotation_matrix = np.matmul(Rz, np.matmul(Ry, Rx))\n",
        "\n",
        "    return rotation_matrix\n",
        "\n",
        "\n",
        "def center_positions(pos):\n",
        "    offset = np.mean(pos, axis=0)\n",
        "    return pos - offset, offset\n",
        "\n",
        "\n",
        "def build_model(args, ckpt=None):\n",
        "    # bond_info = create_water_bond(NUM_OF_ATOMS)\n",
        "    # print(bond_info)\n",
        "    param_dict = {'in_feats': 1,\n",
        "                'encoding_size': args.encoding_size,\n",
        "                'out_feats': 3,\n",
        "                #'bond': bond_info,\n",
        "                'hidden_dim': args.hidden_dim,\n",
        "                'edge_embedding_dim': args.edge_embedding_dim,\n",
        "                'conv_layer': args.conv_layer,\n",
        "                'drop_edge': args.drop_edge,\n",
        "                'use_layer_norm': args.use_layer_norm,\n",
        "                'update_edge': args.update_edge,\n",
        "                }\n",
        "    # small model\n",
        "    # param_dict = {'in_feats': 1,\n",
        "    #               'encoding_size': 512,\n",
        "    #               'out_feats': 3,\n",
        "    #               'bond': bond_info,\n",
        "    #               'hidden_dim': 256,\n",
        "    #               'conv_layer': 5,\n",
        "    #               }\n",
        "\n",
        "    print(\"Using following set of hyper-parameters\")\n",
        "    print(args)\n",
        "\n",
        "    # print(param_dict)\n",
        "    model = WaterMDDynamicBoxNet(**param_dict, expand_edge=args.expand_edge)\n",
        "\n",
        "    if ckpt is not None:\n",
        "        print('Loading model weights from: ', ckpt)\n",
        "        model.load_state_dict((torch.load(ckpt)))\n",
        "    return model\n",
        "\n",
        "\n",
        "class ParticleNetLightning(pl.LightningModule):\n",
        "    def __init__(self, args, num_device=1, epoch_num=100, batch_size=1, learning_rate=3e-4, log_freq=1000,\n",
        "                 model_weights_ckpt=None, scaler_ckpt=None):\n",
        "        super(ParticleNetLightning, self).__init__()\n",
        "        self.pnet_model = build_model(args, model_weights_ckpt)\n",
        "        self.epoch_num = epoch_num\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.num_device = num_device\n",
        "        self.log_freq = log_freq\n",
        "        self.train_data_scaler = StandardScaler()\n",
        "        self.training_mean = np.array([0.])\n",
        "        self.training_var = np.array([1.])\n",
        "\n",
        "        if scaler_ckpt is not None:\n",
        "            self.load_training_stats(scaler_ckpt)\n",
        "\n",
        "        self.cutoff = args.cutoff\n",
        "        self.rotate_aug = args.rotate_aug\n",
        "        self.data_dir = args.data_dir\n",
        "        self.loss_fn = args.loss\n",
        "        self.use_part = args.use_part\n",
        "        assert self.loss_fn in ['mae', 'mse']\n",
        "\n",
        "\n",
        "    def load_training_stats(self, scaler_ckpt):\n",
        "        if scaler_ckpt is not None:\n",
        "            scaler_info = np.load(scaler_ckpt)\n",
        "            self.training_mean = scaler_info['mean']\n",
        "            self.training_var = scaler_info['var']\n",
        "\n",
        "    def forward(self, pos, feat, edge_idx_tsr):\n",
        "        return self.denormalize(self.pnet_model(pos, feat, edge_idx_tsr.long()), self.training_var, self.training_mean)\n",
        "\n",
        "    def build_graph(self, edge_idx):\n",
        "        return self.pnet_model.build_partial_graph(edge_idx)\n",
        "\n",
        "    def denormalize(self, normalized_force, var, mean):\n",
        "        return normalized_force * \\\n",
        "                np.sqrt(var) +\\\n",
        "                mean\n",
        "\n",
        "    def predict_forces(self, feat: torch.Tensor, pos: np.ndarray, box_size):\n",
        "        # enforce periodic boundary\n",
        "        pos = np.mod(pos, box_size)\n",
        "        pos = torch.from_numpy(pos).float().to(feat.device)\n",
        "        pred = self.pnet_model([pos],\n",
        "                               feat,\n",
        "                               [box_size],\n",
        "                               self.cutoff\n",
        "                               )\n",
        "\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "\n",
        "        pred = self.denormalize(pred, self.training_var, self.training_mean)\n",
        "\n",
        "        return pred\n",
        "\n",
        "    def scale_force(self, force, scaler):\n",
        "        b_pnum, dims = force.shape\n",
        "        force_flat = force.reshape((-1, 1))\n",
        "        scaler.partial_fit(force_flat)\n",
        "        force = torch.from_numpy(scaler.transform(force_flat)).float().view(b_pnum, dims)\n",
        "        return force\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        feat, pos_lst, box_size_lst = batch['feat'], batch['pos'], batch['box_size']\n",
        "        gt_lst = batch['forces']\n",
        "\n",
        "        for b in range(len(gt_lst)):\n",
        "            pos, box_size, gt = pos_lst[b], box_size_lst[b], gt_lst[b]\n",
        "            pos, off = center_positions(pos)\n",
        "            R = get_rotation_matrix()\n",
        "            pos = np.matmul(pos, R)\n",
        "            pos += off\n",
        "            box_size = np.matmul(box_size, R)\n",
        "            box_size_lst[b] = box_size\n",
        "            pos = np.mod(pos, box_size)\n",
        "            gt = np.matmul(gt, R)\n",
        "            gt = self.scale_force(gt, self.train_data_scaler).to(feat.device)\n",
        "            pos_lst[b] = torch.from_numpy(pos).float().to(feat.device)\n",
        "            gt_lst[b] = gt\n",
        "        gt = torch.cat(gt_lst, dim=0)\n",
        "\n",
        "        # enforce periodic boundary\n",
        "        pos_lst = [pos + torch.randn_like(pos) * 0.00025 for pos in pos_lst]\n",
        "        pred = self.pnet_model(pos_lst,\n",
        "                               feat,\n",
        "                               box_size_lst,\n",
        "                               self.cutoff\n",
        "                               )\n",
        "        epoch = self.current_epoch\n",
        "        # if epoch > 5:\n",
        "        #     mae = nn.L1Loss()(pred, gt)\n",
        "        # else:\n",
        "        if self.loss_fn == 'mae':\n",
        "            loss = nn.L1Loss()(pred, gt)\n",
        "        else:\n",
        "            loss = nn.MSELoss()(pred, gt)\n",
        "\n",
        "        conservative_loss = (torch.mean(pred)).abs()\n",
        "        loss = loss + LAMBDA2*conservative_loss\n",
        "\n",
        "        self.training_mean = self.train_data_scaler.mean_\n",
        "        self.training_var = self.train_data_scaler.var_\n",
        "\n",
        "        self.log('total loss', loss, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log(f'{self.loss_fn} loss', loss, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log('var', np.sqrt(self.training_var), on_step=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # self.log('regularization', conservative_loss, on_step=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        # optim = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=0.9)\n",
        "        sched = StepLR(optim, step_size=100, gamma=0.001**(100/self.epoch_num))\n",
        "        return [optim], [sched]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        dataset = WaterDataRealLarge(dataset_path=os.path.join(self.data_dir, 'RPBE-data-processed.npz'), use_part=self.use_part)\n",
        "        return DataLoader(dataset, num_workers=2, batch_size=self.batch_size, shuffle=True,\n",
        "                          collate_fn=\n",
        "                          lambda batches: {\n",
        "                              'feat': torch.cat([torch.from_numpy(batch['feat']).float() for batch in batches], dim=0),\n",
        "                              'pos': [batch['pos'] for batch in batches],\n",
        "                              'forces': [batch['forces'] for batch in batches],\n",
        "                              'box_size': [batch['box_size'] for batch in batches],\n",
        "                          })\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        dataset = WaterDataRealLarge(dataset_path=os.path.join(self.data_dir, 'RPBE-data-processed.npz'),\n",
        "                                     mode='test')\n",
        "        return DataLoader(dataset, num_workers=2, batch_size=self.batch_size*2, shuffle=False,\n",
        "                          collate_fn=\n",
        "                          lambda batches: {\n",
        "                              'feat': torch.cat([torch.from_numpy(batch['feat']).float() for batch in batches], dim=0),\n",
        "                              'pos': [batch['pos'] for batch in batches],\n",
        "                              'forces': [batch['forces'] for batch in batches],\n",
        "                              'box_size': [batch['box_size'] for batch in batches],\n",
        "                          })\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        with torch.no_grad():\n",
        "            feat, pos_lst, box_size_lst = batch['feat'], batch['pos'], batch['box_size']\n",
        "            gt_lst = batch['forces']\n",
        "\n",
        "            for b in range(len(gt_lst)):\n",
        "                pos, box_size, gt = pos_lst[b], box_size_lst[b], gt_lst[b]\n",
        "\n",
        "                pos = np.mod(pos, box_size)\n",
        "                gt = self.scale_force(gt, self.train_data_scaler).to(feat.device)\n",
        "                pos_lst[b] = torch.from_numpy(pos).float().to(feat.device)\n",
        "                gt_lst[b] = gt\n",
        "            gt = torch.cat(gt_lst, dim=0)\n",
        "            # enforce periodic boundary\n",
        "            pred = self.pnet_model(pos_lst,\n",
        "                                   feat,\n",
        "                                   box_size_lst,\n",
        "                                   self.cutoff\n",
        "                                   )\n",
        "            mse = nn.MSELoss()(pred, gt)\n",
        "            mae = nn.L1Loss()(pred, gt)\n",
        "\n",
        "            self.training_mean = self.train_data_scaler.mean_\n",
        "            self.training_var = self.train_data_scaler.var_\n",
        "\n",
        "            self.log('val mse', mse, prog_bar=True, logger=True)\n",
        "            self.log('val mae', mae, prog_bar=True, logger=True)\n",
        "\n",
        "\n",
        "class ModelCheckpointAtEpochEnd(pl.Callback):\n",
        "    \"\"\"\n",
        "       Save a checkpoint at epoch end\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            filepath,\n",
        "            save_step_frequency,\n",
        "            prefix=\"checkpoint\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            save_step_frequency: how often to save in steps\n",
        "            prefix: add a prefix to the name, only used if\n",
        "        \"\"\"\n",
        "        self.filepath = filepath\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_epoch_end(self, trainer: pl.Trainer, pl_module: ParticleNetLightning):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        if epoch % self.save_step_frequency == 0 or epoch == pl_module.epoch_num -1:\n",
        "            filename = os.path.join(self.filepath, f\"{self.prefix}_{epoch}.ckpt\")\n",
        "            scaler_filename = os.path.join(self.filepath, f\"scaler_{epoch}.npz\")\n",
        "\n",
        "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "            np.savez(scaler_filename,\n",
        "                     mean=pl_module.training_mean,\n",
        "                     var=pl_module.training_var,\n",
        "                     )\n",
        "            # joblib.dump(pl_module.train_data_scaler, scaler_filename)\n",
        "\n",
        "\n",
        "def train_model(args):\n",
        "    lr = args.lr\n",
        "    num_gpu = args.num_gpu\n",
        "    check_point_dir = args.cp_dir\n",
        "    min_epoch = args.min_epoch\n",
        "    max_epoch = args.max_epoch\n",
        "    weight_ckpt = args.state_ckpt_dir\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    model = ParticleNetLightning(epoch_num=max_epoch,\n",
        "                                 num_device=num_gpu if num_gpu != -1 else 1,\n",
        "                                 learning_rate=lr,\n",
        "                                 model_weights_ckpt=weight_ckpt,\n",
        "                                 batch_size=batch_size,\n",
        "                                 args=args)\n",
        "    cwd = os.getcwd()\n",
        "    model_check_point_dir = os.path.join(cwd, check_point_dir)\n",
        "    os.makedirs(model_check_point_dir, exist_ok=True)\n",
        "    epoch_end_callback = ModelCheckpointAtEpochEnd(filepath=model_check_point_dir, save_step_frequency=50)\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
        "\n",
        "    trainer = Trainer(gpus=num_gpu,\n",
        "                      callbacks=[epoch_end_callback, checkpoint_callback],\n",
        "                      min_epochs=min_epoch,\n",
        "                      max_epochs=max_epoch,\n",
        "                      amp_backend='apex',\n",
        "                      amp_level='O2',\n",
        "                      benchmark=True,\n",
        "                      distributed_backend='ddp',\n",
        "                      )\n",
        "    trainer.fit(model)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--min_epoch', default=800, type=int)\n",
        "    parser.add_argument('--max_epoch', default=800, type=int)\n",
        "    parser.add_argument('--lr', default=1e-4, type=float)\n",
        "    parser.add_argument('--cp_dir', default='./model_ckpt')\n",
        "    parser.add_argument('--state_ckpt_dir', default=None, type=str)\n",
        "\n",
        "    parser.add_argument('--batch_size', default=8, type=int)\n",
        "    parser.add_argument('--encoding_size', default=256, type=int)\n",
        "    parser.add_argument('--hidden_dim', default=128, type=int)\n",
        "    parser.add_argument('--edge_embedding_dim', default=256, type=int)\n",
        "    parser.add_argument('--cutoff', default=9.5, type=float)\n",
        "    parser.add_argument('--conv_layer', default=5, type=int)\n",
        "    parser.add_argument('--drop_edge', action='store_true')\n",
        "    parser.add_argument('--use_layer_norm', action='store_true')\n",
        "    parser.add_argument('--update_edge', action='store_true')\n",
        "    parser.add_argument('--disable_expand_edge', dest='expand_edge', default=True, action='store_false')\n",
        "\n",
        "    parser.add_argument('--disable_rotate_aug', dest='rotate_aug', default=True, action='store_false')\n",
        "    parser.add_argument('--data_dir', default='./md_dataset')\n",
        "    parser.add_argument('--use_part', action='store_true')    # use only part of the training data?\n",
        "    parser.add_argument('--loss', default='mae')\n",
        "    parser.add_argument('--num_gpu', default=-1, type=int)\n",
        "    args = parser.parse_args()\n",
        "    train_model(args)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "NdEEHBV82UfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup model and dataloaders"
      ],
      "metadata": {
        "id": "7MRwYxNF2dbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_dataset(gamdnet_model_filename, gamdnet_official_model_checkpoint_filename, md_filedir):\n",
        "    '''\n",
        "    Load model and MD dataset for SR from input filename and dataset directory.\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    embed_dim = 128\n",
        "    hidden_dim = 128\n",
        "    num_mpnn_layers = 4 # as per paper, for LJ system\n",
        "    num_mlp_layers = 3\n",
        "    num_atom_type_classes = 1 # Ar atoms only\n",
        "    num_edge_types = 1 # non-bonded edges only\n",
        "    num_rbfs = 10 # RBF expansion of interatomic distance vector of each edge to num_rbfs dimensions\n",
        "    gamdnet = GAMDNet(embed_dim, hidden_dim, num_mpnn_layers, num_mlp_layers, num_atom_type_classes, num_edge_types, num_rbfs).to(device)\n",
        "    # Load the weights from 'model.pt'\n",
        "    # Load the checkpoint from 'model.pt'\n",
        "    checkpoint = torch.load(gamdnet_model_filename)\n",
        "    gamdnet.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # Set the model to evaluation mode\n",
        "    gamdnet.eval()\n",
        "\n",
        "    print(\"GAMD model weights loaded successfully.\")\n",
        "    '''\n",
        "    train_data_fraction = 1.0 # select 9k for training\n",
        "    avg_num_neighbors = 20 # criteria for connectivity of atoms for any frame\n",
        "    rotation_aug = False # online rotation augmentation for a frame\n",
        "    # create train data-loader\n",
        "    return_train_data = True\n",
        "    num_input_files = 1# 40#len(os.listdir(md_filedir))\n",
        "    batch_size = num_input_files # number of graphs in a batch\n",
        "    print(\"Loading input files: \", num_input_files)\n",
        "    #print(\"Files are: \", os.listdir(md_filedir))\n",
        "    dataset = MDDataset(md_filedir, rotation_aug, avg_num_neighbors, train_data_fraction, return_train_data)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
        "    print(\"Dataloader initialized.\")\n",
        "\n",
        "\n",
        "    print(\"Loading official GAMDNET...\")\n",
        "    param_dict = {\n",
        "                'encoding_size': 128,\n",
        "                'out_feats': 3,\n",
        "                'hidden_dim': 128,\n",
        "                'edge_embedding_dim': 128,\n",
        "                'conv_layer': 4,\n",
        "                'drop_edge': False,\n",
        "                'use_layer_norm': True,\n",
        "                'box_size': 27.27,\n",
        "                }\n",
        "    gamdnet_official = SimpleMDNetNew(**param_dict).to(device)\n",
        "    checkpoint = torch.load(gamdnet_official_model_checkpoint_filename, map_location='cuda:0')\n",
        "\n",
        "    state_dict_original = checkpoint['state_dict']\n",
        "\n",
        "    # Define the prefix to remove\n",
        "    prefix_to_remove = 'pnet_model.'\n",
        "\n",
        "    # Create a new dictionary with updated keys\n",
        "    state_dict_without_prefix = {\n",
        "        key[len(prefix_to_remove):]: value\n",
        "        for key, value in state_dict_original.items()\n",
        "        if key.startswith(prefix_to_remove)\n",
        "    }\n",
        "\n",
        "    gamdnet_official.load_state_dict(state_dict_without_prefix)\n",
        "\n",
        "    print(\"GAMD official model weights loaded successfully.\")\n",
        "\n",
        "    gamdnet_official.eval()\n",
        "\n",
        "    #return gamdnet, None, dataloader\n",
        "    return None, gamdnet_official, dataloader\n"
      ],
      "metadata": {
        "id": "hp-jCYkv2wZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collect edge messages and LJ potential for analysis"
      ],
      "metadata": {
        "id": "EKQ6i7iM2jjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_edge_msg_dict(self, gamdnet_official, dataloader):\n",
        "    msg_force_dict = {}\n",
        "    # run inference over the input batched graph from dataloader\n",
        "    # record aggregate edge messages and force ground truths for each node in output dictionary\n",
        "    for pos, edge_index_list, force_gt in dataloader:\n",
        "        with torch.no_grad():  # Disable gradient calculation for inference\n",
        "            # our implementation\n",
        "            '''\n",
        "            force_pred = gamdnet(pos, edge_index_list)  # Forward pass through the model\n",
        "            msg_force_dict['edge_messages'] = gamdnet.mpnn_mlps.mp_blocks[3].edge_message_neigh_center\n",
        "            evaluate(force_gt, force_pred)\n",
        "            '''\n",
        "            # official implementation\n",
        "            force_pred_official = gamdnet_official([pos],\n",
        "                               [edge_index_list])\n",
        "            msg_force_dict['edge_messages'] = gamdnet_official.graph_conv.conv[-1].edge_message_neigh_center\n",
        "            msg_force_dict['aggregate_edge_messages'] = gamdnet_official.graph_conv.conv[-1].aggregate_edge_messages\n",
        "\n",
        "            print(\"Results from official model:\")\n",
        "            evaluate(force_gt, force_pred_official)\n",
        "\n",
        "            # record messages for SR\n",
        "            lj_force, lj_potential, radial_distance, valid_indices, dx, dy, dz = compute_lj_force_and_potential(pos, edge_index_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # remove nans\n",
        "            lj_force = lj_force[valid_indices]\n",
        "            msg_force_dict['edge_messages'] = msg_force_dict['edge_messages'][valid_indices]\n",
        "            msg_force_dict['radial_distance'] = radial_distance[valid_indices]\n",
        "\n",
        "            # Define a threshold for closeness to zero\n",
        "            threshold = 1e-5\n",
        "\n",
        "            # Identify rows where all elements are close to zero\n",
        "            close_to_zero_rows = (torch.abs(lj_force) < threshold).all(dim=1)\n",
        "\n",
        "            # Create a mask for rows that are NOT close to zero\n",
        "            non_zero_rows_mask = ~close_to_zero_rows\n",
        "\n",
        "\n",
        "\n",
        "            print(\"Before zero row removal, \", lj_force.shape, msg_force_dict['edge_messages'].shape)\n",
        "            # Filter the tensor to keep only the non-zero rows\n",
        "            lj_force = lj_force[non_zero_rows_mask]\n",
        "            msg_force_dict['edge_messages'] = msg_force_dict['edge_messages'][non_zero_rows_mask]\n",
        "            msg_force_dict['force_gt'] = lj_force # [num_particle * batch_size, 3]\n",
        "            msg_force_dict['radial_distance'] = msg_force_dict['radial_distance'][non_zero_rows_mask]\n",
        "            msg_force_dict['dx'] = dx[valid_indices][non_zero_rows_mask]\n",
        "            msg_force_dict['dy'] = dy[valid_indices][non_zero_rows_mask]\n",
        "            msg_force_dict['dz'] = dz[valid_indices][non_zero_rows_mask]\n",
        "            msg_force_dict['potential_gt'] = lj_potential[valid_indices][non_zero_rows_mask]\n",
        "            msg_force_dict['net_force_gt'] = force_gt\n",
        "            msg_force_dict['pos'] = pos\n",
        "            msg_force_dict['node_embeddings'] = gamdnet_official.graph_conv.conv[-1].node_embeddings\n",
        "        break # run dataloader only once\n",
        "    return sr_inputs, sr_outputs"
      ],
      "metadata": {
        "id": "tmLs8cPy3Fi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sr_inputs_and_outputs(self, sr_config):\n",
        "\n",
        "  if sr_config['use_pregenerated_edge_msgs?'] == 'n':\n",
        "    return generate_edge_msg_dict(self, gamdnet_official, dataloader)\n",
        "\n",
        "  # else, read edge messages from input file\n",
        "  using PyCall\n",
        "\n",
        "  # Import the pickle module from Python\n",
        "  @pyimport pickle\n",
        "\n",
        "  # Function to load a pickle file\n",
        "  function load_pickle(filename)\n",
        "      try\n",
        "          # Read the entire content of the file as bytes\n",
        "          bytes = read(filename)  # This reads the file content into a Vector{UInt8}\n",
        "\n",
        "          # Use pickle.loads() to load the data from bytes\n",
        "          return pickle.loads(pybytes(bytes))  # Convert bytes to a Python bytes object\n",
        "      catch e\n",
        "          println(\"Error loading pickle file: \", e)\n",
        "          return nothing  # Return nothing if there was an error\n",
        "      end\n",
        "  end\n",
        "\n",
        "  # Example usage\n",
        "  #msg_force_dict_pkl_filename = \"/content/msg_force_dict_epoch=29-step=270000_edge_msg_constrained_std_trained_over_9k_samples_our_run_5.pkl\"\n",
        "  #msg_force_dict_pkl_filename = \"/content/msg_force_dict_epoch=29-step=270000_edge_msg_constrained_std_trained_over_9k_samples_our_run_5_on_gamd_dataset.pkl\"\n",
        "  #msg_force_dict_pkl_filename = \"/content/msg_force_dict_epoch=29-step=135000_edge_msg_constrained_std_trained_over_4.5k_samples_custom_potential.pkl\"\n",
        "  msg_force_dict_pkl_filename = \"msg_force_dict_epoch=39-step=360000_edge_msg_constrained_std.pkl\"\n",
        "  msg_force_dict = load_pickle(msg_force_dict_pkl_filename)\n",
        "\n",
        "  # Displaying the loaded data\n",
        "  if msg_force_dict !== nothing\n",
        "      println(\"Loaded data: \", msg_force_dict.keys)\n",
        "  else\n",
        "      println(\"Failed to load data.\")\n",
        "  end\n",
        "\n",
        "  return sr_inputs, sr_outputs"
      ],
      "metadata": {
        "id": "vmYgY32h6QJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define SR inputs and outputs"
      ],
      "metadata": {
        "id": "vvC-JXEZUMxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sr_input_outputs(sr_inputs, sr_outputs, sr_config):\n",
        "  using DataFrames\n",
        "\n",
        "  X = msg_force_dict[\"radial_distance\"].cpu().numpy()\n",
        "\n",
        "\n",
        "  edge_messages_julia = msg_force_dict[\"edge_messages\"].cpu().numpy()\n",
        "  Y = edge_messages_julia[:, 3] # Get all rows and the first column (Julia indexing starts at 1)\n",
        "\n",
        "  # Create a mask\n",
        "  mask = (0 .<= Y) #.& (10.0 .>= X)\n",
        "\n",
        "  # Filter X and Y using the mask\n",
        "  X = X[mask]\n",
        "  Y = Y[mask]\n",
        "\n",
        "  # Calculate the standard deviation of Y values\n",
        "  std_dev_Y = np.std(Y)\n",
        "\n",
        "  # Print or return the standard deviation\n",
        "  println(\"Standard Deviation of Y values: \", std_dev_Y)\n",
        "\n",
        "  # Plotting Y as a function of X\n",
        "  plot(X, Y, seriestype = :scatter, label = \"Edge messages\", xlabel = \"Radial Distance (X)\", ylabel = \"Edge messages (Y)\", title = \"Edge messages vs Radial Distance\", legend = true)\n",
        "\n",
        "\n",
        "  \"\"\"# Downsample edge messages (to map it to a function)\"\"\"\n",
        "\n",
        "  using Statistics\n",
        "  # Define the window size for downsampling\n",
        "  window_size = 0.1  # Adjust this value as needed\n",
        "\n",
        "  # Create arrays to store downsampled results\n",
        "  downsampled_X = Float64[]\n",
        "  downsampled_Y = Float64[]\n",
        "\n",
        "  # Get unique X values for downsampling\n",
        "  unique_X_values = sort(unique(X))\n",
        "\n",
        "  # Iterate over unique X values and compute averages within the window\n",
        "  for x in unique_X_values\n",
        "      # Find indices of Y values within the window around x\n",
        "      indices_in_window = findall((X .>= (x - window_size / 2)) .& (X .<= (x + window_size / 2)))\n",
        "\n",
        "      if !isempty(indices_in_window)\n",
        "          # Calculate average of Y values in this window\n",
        "          avg_Y = mean(Y[indices_in_window])\n",
        "\n",
        "          # Check if x is already in downsampled_X before adding it\n",
        "          if x in downsampled_X\n",
        "              continue  # Skip if x is already present\n",
        "          else\n",
        "              # Append results to downsampled arrays\n",
        "              push!(downsampled_X, x)\n",
        "              push!(downsampled_Y, avg_Y)\n",
        "          end\n",
        "      end\n",
        "  end\n",
        "\n",
        "  # Convert results to arrays if needed\n",
        "  X = collect(downsampled_X)\n",
        "  Y = collect(downsampled_Y)\n",
        "\n",
        "  # Print or return the downsampled results\n",
        "  println(\"Downsampled X: \", X)\n",
        "  println(\"Downsampled Y: \", Y)\n",
        "\n",
        "  # Plotting downsampled Y as a function of downsampled X\n",
        "  plot(X, Y, seriestype = :scatter, label = \"Downsampled Edge messages\", xlabel = \"Radial Distance (X)\", ylabel = \"Average Edge messages (Y)\", title = \"Downsampled Edge messages vs Radial Distance\", legend = true)\n",
        "\n",
        "  return sr_inputs, sr_outputs\n"
      ],
      "metadata": {
        "id": "JTe7ogU158VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test for linearity of fit between pair potential and pair-force vs edge messages"
      ],
      "metadata": {
        "id": "aA7F54QrbUcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_lj_force_and_potential(pos, edge_index_list):\n",
        "\n",
        "    center_node_idx = edge_index_list[0, :]\n",
        "    neigh_node_idx = edge_index_list[1, :]\n",
        "\n",
        "    neigh_node_pos = pos[neigh_node_idx]\n",
        "    center_node_pos = pos[center_node_idx]\n",
        "\n",
        "    # Calculate the distance vector\n",
        "    r_vec = neigh_node_pos - center_node_pos  # Shape: [n, 3]\n",
        "\n",
        "    # Calculate the distance (magnitude)\n",
        "    r = torch.norm(r_vec, dim=1).unsqueeze(1)  # Shape: [n, 1]\n",
        "\n",
        "    epsilon = 0.238\n",
        "    sigma = 3.4\n",
        "    force_magnitude = 48 * epsilon * (\n",
        "        ((sigma ** 12) / (r ** 13)) -\n",
        "        ((sigma ** 6) / (r ** 7))\n",
        "    )  # Shape: [n, 1]\n",
        "\n",
        "    # Calculate the force vector (directed)\n",
        "    force_vector = force_magnitude * (r_vec / r)  # Shape: [n, 3]\n",
        "\n",
        "    potential_magnitude = 4 * epsilon * ((sigma / r) ** 12 - (sigma / r) ** 6)\n",
        "\n",
        "    potential_vector = potential_magnitude * (r_vec / r)\n",
        "\n",
        "    nan_mask = torch.isnan(force_vector).any(dim=1)\n",
        "    valid_indices = ~nan_mask\n",
        "\n",
        "    dx = r_vec[:, 0]\n",
        "    dy = r_vec[:, 1]\n",
        "    dz = r_vec[:, 2]\n",
        "\n",
        "    return force_vector, potential_vector, r, valid_indices, dx, dy, dz"
      ],
      "metadata": {
        "id": "bEUltLcG3bdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "msg_most_imp = None\n",
        "expected_forces = None\n",
        "expected_potentials = None\n",
        "\n",
        "def percentile_sum(x):\n",
        "    x = x.ravel()\n",
        "    bot = x.min()\n",
        "    top = np.percentile(x, 90)\n",
        "    msk = (x>=bot) & (x<=top)\n",
        "    frac_good = (msk).sum()/len(x)\n",
        "    return x[msk].sum()/frac_good\n",
        "\n",
        "\n",
        "def linear_transformation_3d_force(alpha):\n",
        "\n",
        "    global msg_most_imp\n",
        "    global expected_forces\n",
        "\n",
        "    lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1] + alpha[2] * expected_forces[:, 2]) + alpha[3]\n",
        "    lincomb2 = (alpha[0+4] * expected_forces[:, 0] + alpha[1+4] * expected_forces[:, 1] + alpha[2+4] * expected_forces[:, 2]) + alpha[3+4]\n",
        "    lincomb3 = (alpha[0+8] * expected_forces[:, 0] + alpha[1+8] * expected_forces[:, 1] + alpha[2+8] * expected_forces[:, 2]) + alpha[3+8]\n",
        "\n",
        "    score = (\n",
        "        percentile_sum(np.square(msg_most_imp[:, 0] - lincomb1)) +\n",
        "        percentile_sum(np.square(msg_most_imp[:, 1] - lincomb2)) +\n",
        "        percentile_sum(np.square(msg_most_imp[:, 2] - lincomb3))\n",
        "    )/3.0\n",
        "    '''\n",
        "    score = np.mean([np.abs(msg_most_imp[:, 0] - lincomb1) +\n",
        "        np.abs(msg_most_imp[:, 1] - lincomb2) +\n",
        "        np.abs(msg_most_imp[:, 2] - lincomb3)]) / 3.0\n",
        "\n",
        "    print(\"Alpha now is: \", alpha)\n",
        "    print(\"Score now is: \", score)\n",
        "    '''\n",
        "    return score\n",
        "\n",
        "\n",
        "def out_linear_transformation_3d_force(alpha):\n",
        "\n",
        "    global msg_most_imp\n",
        "    global expected_forces\n",
        "\n",
        "    lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1] + alpha[2] * expected_forces[:, 2]) + alpha[3]\n",
        "    lincomb2 = (alpha[0+4] * expected_forces[:, 0] + alpha[1+4] * expected_forces[:, 1] + alpha[2+4] * expected_forces[:, 2]) + alpha[3+4]\n",
        "    lincomb3 = (alpha[0+8] * expected_forces[:, 0] + alpha[1+8] * expected_forces[:, 1] + alpha[2+8] * expected_forces[:, 2]) + alpha[3+8]\n",
        "    return lincomb1, lincomb2, lincomb3\n",
        "\n",
        "\n",
        "def linear_transformation_3d_potential(alpha):\n",
        "\n",
        "    global msg_most_imp\n",
        "    global expected_potentials\n",
        "\n",
        "    lincomb1 = (alpha[0] * expected_potentials[:, 0] + alpha[1] * expected_potentials[:, 1] + alpha[2] * expected_potentials[:, 2]) + alpha[3]\n",
        "    lincomb2 = (alpha[0+4] * expected_potentials[:, 0] + alpha[1+4] * expected_potentials[:, 1] + alpha[2+4] * expected_potentials[:, 2]) + alpha[3+4]\n",
        "    lincomb3 = (alpha[0+8] * expected_potentials[:, 0] + alpha[1+8] * expected_potentials[:, 1] + alpha[2+8] * expected_potentials[:, 2]) + alpha[3+8]\n",
        "\n",
        "\n",
        "    score = (\n",
        "        percentile_sum(np.square(msg_most_imp[:, 0] - lincomb1)) +\n",
        "        percentile_sum(np.square(msg_most_imp[:, 1] - lincomb2)) +\n",
        "        percentile_sum(np.square(msg_most_imp[:, 2] - lincomb3))\n",
        "    )/3.0\n",
        "    '''\n",
        "    score = np.mean([np.abs(msg_most_imp[:, 0] - lincomb1) + np.abs(msg_most_imp[:, 1] - lincomb2) + np.abs(msg_most_imp[:, 2] - lincomb3)]) / 3.0\n",
        "\n",
        "    print(\"Alpha now is: \", alpha)\n",
        "    print(\"Score now is: \", score)\n",
        "    '''\n",
        "    return score\n",
        "\n",
        "\n",
        "def out_linear_transformation_3d_potential(alpha):\n",
        "\n",
        "    global msg_most_imp\n",
        "    global expected_potentials\n",
        "\n",
        "    lincomb1 = (alpha[0] * expected_potentials[:, 0] + alpha[1] * expected_potentials[:, 1] + alpha[2] * expected_potentials[:, 2]) + alpha[3]\n",
        "    lincomb2 = (alpha[0+4] * expected_potentials[:, 0] + alpha[1+4] * expected_potentials[:, 1] + alpha[2+4] * expected_potentials[:, 2]) + alpha[3+4]\n",
        "    lincomb3 = (alpha[0+8] * expected_potentials[:, 0] + alpha[1+8] * expected_potentials[:, 1] + alpha[2+8] * expected_potentials[:, 2]) + alpha[3+8]\n",
        "\n",
        "    print(\"alphas: \", alpha)\n",
        "    return lincomb1, lincomb2,  lincomb3\n",
        "\n",
        "\n",
        "\n",
        "def are_edge_msgs_gt_force_correlated(msg_force_dict):\n",
        "    '''\n",
        "    msg_force_dict: {'edge_messages': [total_edges, emb_dim], 'gt_force': [total_edges, 3]}\n",
        "    '''\n",
        "    global msg_most_imp\n",
        "    global expected_forces\n",
        "\n",
        "    print(\"edge message shape: \", msg_force_dict['edge_messages'].shape)\n",
        "    print(\"force shape: \", msg_force_dict['force_gt'].shape)\n",
        "\n",
        "    # Calculate variance for each component of agg_msg across all samples\n",
        "    msg_comp_std = torch.std(msg_force_dict['edge_messages'], axis=0)  # Variance for each component\n",
        "\n",
        "    # Step 3: Get top-3 indices based on variance\n",
        "    top_std_indices = torch.argsort(msg_comp_std)[-3:]  # Get indices of top-3 components with maximum variance\n",
        "\n",
        "    # Prepare data for linear regression using top-3 components as output variables\n",
        "    msg_most_imp = msg_force_dict['edge_messages'][:, top_std_indices].cpu()  # Select only the top-3 components\n",
        "\n",
        "\n",
        "    # normalize the messages\n",
        "    #msg_most_imp = ((msg_most_imp - torch.mean(msg_most_imp, axis=0)) / torch.std(msg_most_imp, axis=0)).cpu()\n",
        "\n",
        "    expected_forces = msg_force_dict['force_gt'].cpu()\n",
        "\n",
        "\n",
        "    dim = 3\n",
        "    min_result = minimize(linear_transformation_3d_force, np.ones(dim**2 + dim), method='Powell')\n",
        "\n",
        "    print(\"Fit score: \", min_result.fun/msg_force_dict['edge_messages'].shape[0])\n",
        "\n",
        "\n",
        "    # Visualize the fit\n",
        "    for i in range(dim):\n",
        "        px = out_linear_transformation_3d_force(min_result.x)[i]\n",
        "        py = msg_most_imp[:, i]\n",
        "        plt.scatter(px, py)\n",
        "        plt.show()\n",
        "\n",
        "    are_correlated = False\n",
        "    return are_correlated, msg_most_imp\n",
        "\n",
        "\n",
        "\n",
        "def are_edge_msgs_gt_potential_correlated(msg_force_dict):\n",
        "    '''\n",
        "    msg_force_dict: {'edge_messages': [total_edges, emb_dim], 'gt_force': [total_edges, 3]}\n",
        "    '''\n",
        "    global msg_most_imp\n",
        "    global expected_potentials\n",
        "\n",
        "    print(\"edge message shape: \", msg_force_dict['edge_messages'].shape)\n",
        "    print(\"force shape: \", msg_force_dict['potential_gt'].shape)\n",
        "\n",
        "    # Calculate variance for each component of agg_msg across all samples\n",
        "    msg_comp_std = torch.std(msg_force_dict['edge_messages'], axis=0)  # Variance for each component\n",
        "\n",
        "    # Step 3: Get top-3 indices based on variance\n",
        "    top_std_indices = torch.argsort(msg_comp_std)[-3:]  # Get indices of top-3 components with maximum variance\n",
        "\n",
        "    # Prepare data for linear regression using top-3 components as output variables\n",
        "    msg_most_imp = msg_force_dict['edge_messages'][:, top_std_indices].cpu()  # Select only the top-3 components\n",
        "\n",
        "\n",
        "    # normalize the messages\n",
        "    #msg_most_imp = ((msg_most_imp - torch.mean(msg_most_imp, axis=0)) / torch.std(msg_most_imp, axis=0)).cpu()\n",
        "\n",
        "    expected_potentials = msg_force_dict['potential_gt'].cpu()\n",
        "\n",
        "\n",
        "    dim = 3\n",
        "    min_result = minimize(linear_transformation_3d_potential, np.ones(dim**2 + dim), method='Powell')\n",
        "\n",
        "    print(\"Fit score: \", min_result.fun/msg_force_dict['edge_messages'].shape[0])\n",
        "\n",
        "    # Visualize the fit\n",
        "    for i in range(dim):\n",
        "        px = out_linear_transformation_3d_potential(min_result.x)[i]\n",
        "        py = msg_most_imp[:, i]\n",
        "        plt.scatter(px, py)\n",
        "        plt.show()\n",
        "\n",
        "    are_correlated = False\n",
        "    return are_correlated, msg_most_imp\n"
      ],
      "metadata": {
        "id": "j6O5pFeo3Qyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train SR"
      ],
      "metadata": {
        "id": "Snqt-4ZrUO75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regress_edge_message_equations(sr_inputs, sr_outputs, sr_config):\n",
        "  using SymbolicRegression\n",
        "  function lj_potential_structure((; attr_func, rep_func), (rad, ))\n",
        "    _attr_func = attr_func(rad)^-12\n",
        "    _rep_func = rep_func(rad)^-6\n",
        "\n",
        "    out = map((attr_func_i, rep_func_i) -> (attr_func_i - rep_func_i), _attr_func.x, _rep_func.x)\n",
        "    return ValidVector(out, _attr_func.valid && _rep_func.valid)\n",
        "  end\n",
        "  lj_structure = TemplateStructure{(:attr_func, :rep_func)}(lj_potential_structure)\n",
        "\n",
        "  elementwise_loss = ((x1), (y1)) -> abs(y1 - x1)\n",
        "\n",
        "  using MLJBase\n",
        "\n",
        "  model = SRRegressor(;\n",
        "      niterations=10000,\n",
        "      selection_method=SymbolicRegression.MLJInterfaceModule.choose_best,\n",
        "      binary_operators=(*, /),\n",
        "      maxsize=25,\n",
        "      elementwise_loss=elementwise_loss,\n",
        "      #expression_type=TemplateExpression,\n",
        "      # Note - this is where we pass custom options to the expression type:\n",
        "      #expression_options=(; structure = lj_structure),\n",
        "      batching=true,\n",
        "  )\n",
        "\n",
        "\n",
        "  mach = machine(model, X, Y)\n",
        "  fit!(mach)\n",
        "  return mach"
      ],
      "metadata": {
        "id": "YbyefMk76JXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot pred vs gt curve for best SR equation"
      ],
      "metadata": {
        "id": "OsoCkDcRUTaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pred_vs_gt_sr(mach):\n",
        "  r = report(mach)\n",
        "  idx = r.best_idx\n",
        "  best_expr = r.equations[idx]\n",
        "  print(\"Best equation: \", best_expr)\n",
        "  best_attr = get_contents(best_expr).attr_func\n",
        "  best_rep = get_contents(best_expr).rep_func\n",
        "\n",
        "  print(\"\\nAttr term: \", best_attr)\n",
        "  print(\"\\nRep term: \", best_rep)\n",
        "\n",
        "  y_pred = predict(mach, X)\n",
        "  # Plotting Y as a function of X\n",
        "  plot(y_pred, Y, seriestype = :scatter, label = \"GT vs Pred (x-axis)\", xlabel = \"Pred\", ylabel = \"GT\", title = \"GT vs Pred\", legend = true)\n",
        "  savefig(\"pred_vs_gt_without_temp_exp.png\")"
      ],
      "metadata": {
        "id": "xDeXAKBE6bON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm alignment of GNNs ?"
      ],
      "metadata": {
        "id": "nVYWehfBUJkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benefits of algo. alignment ?"
      ],
      "metadata": {
        "id": "zWIop2H7Uh74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAMD-SR class for setting up the E2E process"
      ],
      "metadata": {
        "id": "4N5ktJC7GI2h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApCGIefqSwMT"
      },
      "outputs": [],
      "source": [
        "class GAMDSR(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def run(self, question_dict):\n",
        "\n",
        "    # parse the question dict to setup pipline to answer that question.\n",
        "    # setup dataset, dataloader and GAMD model\n",
        "    dataset_type = self.dataset_config.dataset_type\n",
        "    generate_dataset = self.dataset_config.generate_dataset\n",
        "    dataset_dir = self.dataset_config.dataset_dir\n",
        "    openmm_config = self.dataset_config.openmm_config\n",
        "\n",
        "    if dataset_type == 'lj':\n",
        "      if generate_dataset:\n",
        "        raw_dataset_dir = generate_lj_dataset(openmm_config)\n",
        "      else:\n",
        "        raw_dataset_dir = dataset_dir\n",
        "      dataset = LJDataset(raw_dataset_dir)\n",
        "      dataloader = Dataloader()\n",
        "      model = SimpleMDNet(gamd_model_config)\n",
        "      sr_inputs = dict(radial_dist, dx, dy, dz) # TODO\n",
        "    elif dataset_type == 'tip3p':\n",
        "      if generate_dataset:\n",
        "        raw_dataset_dir = generate_tip3p_dataset(openmm_config)\n",
        "      else:\n",
        "        raw_dataset_dir = dataset_dir\n",
        "      dataset = WarerDatasetNew(raw_dataset_dir)\n",
        "      dataloader = Dataloader(dataset)\n",
        "      model = WaterMDNet(gamd_model_config)\n",
        "      sr_inputs = dict(radial_dist, dx, dy, dz, theta, charge1, charge2) # TODO\n",
        "    elif dataset_type == 'tip4p':\n",
        "      if generate_dataset:\n",
        "        raw_dataset_dir = generate_tip4p_dataset(openmm_config)\n",
        "      else:\n",
        "        raw_dataset_dir = dataset_dir\n",
        "      dataset = WaterDatasetNew(raw_dataset_dir)\n",
        "      dataloader = Dataloader(dataset)\n",
        "      model = WaterMDNet(gamd_model_config)\n",
        "      sr_inputs = dict(radial_dist, dx, dy, dz, theta, charge1, charge2) # TODO\n",
        "    else:\n",
        "      if generate_dataset:\n",
        "        raw_dataset_dir = generate_dft_dataset(openmm_config)\n",
        "      else:\n",
        "        raw_dataset_dir = dataset_dir\n",
        "      dataset = WaterDatasetLarge(raw_dataset_dir)\n",
        "      dataloader = Dataloader(dataset)\n",
        "      model = WaterMDNetDynamic(gamd_model_config)\n",
        "      sr_inputs = dict(radial_dist, dx, dy, dz, theta, charge1, charge2) # TODO\n",
        "    if gamd_model_config.model == 'train':\n",
        "      model.fit()\n",
        "    if gamd_model_config.mode == 'eval':\n",
        "      model.load_state_dict(gamd_model_config.checkpoint_dir)\n",
        "\n",
        "\n",
        "    # setup SR model\n",
        "    sr_inputs, sr_outputs = load_sr_inputs_and_outputs(sr_config)\n",
        "    sr_inputs_preprocessed, sr_outputs_preprocessed = preprocess_sr_input_outputs(sr_inputs, sr_outputs, sr_config)\n",
        "\n",
        "    # check for linearity hypothesis\n",
        "    fit_score_potential = are_edge_msg_pair_potential_correlated(sr_inputs, dataset_type)\n",
        "    fit_score_force = are_edge_msg_pair_force_correlated(sr_inputs, dataset_type)\n",
        "\n",
        "    print(\"Fit score for pair potential is: \", fit_score_potential)\n",
        "    print(\"Fit score for pair force is: \", fit_score_force)\n",
        "\n",
        "\n",
        "    sr_checkpoint = regress_edge_message_equations(sr_inputs_preprocessed, sr_outputs_preprocessed, sr_config)\n",
        "\n",
        "\n",
        "    return model_checkpoint, sr_checkpoint\n",
        "\n",
        "  def compute_answer(self, eval_config_dict, model_checkpoint, sr_checkpoint, iid_dataset,\n",
        "                     ood_dataset_config, output_dir):\n",
        "    if eval_config['evaluate?'] == 'y':\n",
        "      evaluate_metric(mode_checkpoint, ood_dataset_config)  # report generalization performance\n",
        "\n",
        "    # report SR performance\n",
        "   plot_pred_vs_gt_sr(sr_checkpoint)\n",
        "\n",
        "\n",
        "\n",
        "  def evaluate(self, questions_dict):\n",
        "    for question in questions_dict:\n",
        "      model_checkpoint, sr_checkpoint = self.run(question)\n",
        "      compute_answer(question['eval_config'], model_checkpoint, sr_checkpoint, iid_dataset, ood_dataset, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1_dict = {\n",
        "    'dataset_config': {\n",
        "                        'dataset_type': 'LJ',\n",
        "                        'generate_dataset?': 'n',\n",
        "                        'pregenerated_dataset_dirs':{'train_val_dir':'top',\n",
        "                                                     'ood_data_dir': 'top_ood'},\n",
        "                        'openmm_config':{\n",
        "                                        'train_val_config':\n",
        "                                                          {\n",
        "                                                              'num_particles': 258,\n",
        "                                                              'particle_type' : 'Ar',\n",
        "                                                              'num_iters': 1000\n",
        "                                                              'steps_per_iter': 50\n",
        "                                                              'reduced_density' : 0.05,\n",
        "                                                              'box_size':27.27,\n",
        "                                                          'rad_cutoff': 10.2,\n",
        "                                                          'custom_potential_expression': 'n',\n",
        "                                                            'output_dir': 'top/run_1',\n",
        "                                                          },\n",
        "                                      'ood_config':\n",
        "                                                  {\n",
        "                                                      'num_particles': 1000,\n",
        "                                                      'particle_type' : 'Ne',\n",
        "                                                      'num_iters': 1000\n",
        "                                                      'steps_per_iter': 50\n",
        "                                                      'reduced_density' : 0.5,\n",
        "                                                      'rad_cutoff': 15.2,\n",
        "                                                      'custom_potential_expression': 'n',\n",
        "                                                      'output_dir': 'top_ood/run_1',\n",
        "                                                      }\n",
        "                                    },\n",
        "                        'export_movie?': 'y',\n",
        "                        'movie_simulation_dir': 'top/run_1',\n",
        "                        'movie_sim_frame_range': [500, 999],\n",
        "                        'movie_output_dir':'sim_movies/run_1/',\n",
        "                    },\n",
        "    'GAMD_config': {\n",
        "        'train_model?': 'n',\n",
        "        'pretrained_model_checkpoint_dir': 'gamd_model_ckpt_dir/version_1/',\n",
        "        'model_param_config':{\n",
        "                  'in_feats' : 1 ,\n",
        "                 'encoding_size' : 128,\n",
        "                 'out_feats' : 3 ,\n",
        "                 'box_size' : 27.27 ,\n",
        "                 'bond' :None,       #\n",
        "                 'hidden_dim':128,\n",
        "                 'conv_layer':4,\n",
        "                 'edge_embedding_dim':128,\n",
        "                 'dropout':0.1,\n",
        "                 'drop_edge':True,\n",
        "                 'use_layer_norm': False,\n",
        "                            }\n",
        "        'model_hyperparam_config':{\n",
        "                                'loss_fn': 'constrained_edge_std',\n",
        "                                'lr': 0.00001,\n",
        "                                'reg_factor': 0.001,\n",
        "                                'n_epochs': 30,\n",
        "                                'batch_size': 1,\n",
        "                                }\n",
        "                },\n",
        "    'SR_config':{\n",
        "                'input_config':{\n",
        "                                'preprocessing_pipeline': ['avg_filter']\n",
        "                              },\n",
        "                'sr_regressor_config':{\n",
        "                                      'n_iter': 1000,\n",
        "                                      'unary_operators': [],\n",
        "                                      'binary_operators': (*, /),\n",
        "                                      'custom_operators': ,\n",
        "                                      'maxsize':25,\n",
        "                                      'batching':'True',\n",
        "                                      'template_expression':'TemplateExpression',\n",
        "                                      'custom_objective':'((x1), (y1)) -> abs(y1 - x1)',\n",
        "                                      'model_selection_criteria':'SymbolicRegression.MLJInterfaceModule.choose_best',\n",
        "                                      }\n",
        "              'sr_model_checkpoint_dir': 'sr_model_ckpt_dir/run_1',\n",
        "              },\n",
        "    'eval_config':{\n",
        "                  'evalute': 'n',\n",
        "                  'metric': 'MAE',\n",
        "                  }\n",
        "}"
      ],
      "metadata": {
        "id": "XvzNHOGUiFY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Whether the edge messages of GAMD trained on original dataset with constrained msg. std loss correlate with LJ potential function with unguided SR?"
      ],
      "metadata": {
        "id": "tauU5qnpV1tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamd_sr = GAMDSR()\n",
        "gamd_sr.evaluate([question1_dict])"
      ],
      "metadata": {
        "id": "KNXozZQXabi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. Whether the edge messages of GAMD trained on run_5 dataset with constrained msg. std loss correlate with LJ potential function with unguided SR?"
      ],
      "metadata": {
        "id": "HhG1i61iWWF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question2_dict = {\n",
        "    'dataset_config': {\n",
        "                        'dataset_type': 'LJ',\n",
        "                        'generate_dataset?': 'n',\n",
        "                        'pregenerated_dataset_dirs':{'train_val_dir':'run_5',\n",
        "                                                     'ood_data_dir': 'our_runs_ood'},\n",
        "                        'openmm_config':{\n",
        "                                        'train_val_config':\n",
        "                                                          {\n",
        "                                                              'num_particles': 258,\n",
        "                                                              'particle_type' : 'Ar',\n",
        "                                                              'num_iters': 1000\n",
        "                                                              'steps_per_iter': 50\n",
        "                                                              'reduced_density' : 0.05,\n",
        "                                                              'box_size':27.27,\n",
        "                                                          'rad_cutoff': 10.2,\n",
        "                                                          'custom_potential_expression': 'n',\n",
        "                                                            'output_dir': 'top/run_1',\n",
        "                                                          },\n",
        "                                      'ood_config':\n",
        "                                                  {\n",
        "                                                      'num_particles': 1000,\n",
        "                                                      'particle_type' : 'Ne',\n",
        "                                                      'num_iters': 1000\n",
        "                                                      'steps_per_iter': 50\n",
        "                                                      'reduced_density' : 0.5,\n",
        "                                                      'rad_cutoff': 15.2,\n",
        "                                                      'custom_potential_expression': 'n',\n",
        "                                                      'output_dir': 'top_ood/run_1',\n",
        "                                                      }\n",
        "                                    },\n",
        "                        'export_movie?': 'y',\n",
        "                        'movie_simulation_dir': 'top/run_1',\n",
        "                        'movie_sim_frame_range': [500, 999],\n",
        "                        'movie_output_dir':'sim_movies/run_1/',\n",
        "                    },\n",
        "    'GAMD_config': {\n",
        "        'train_model?': 'n',\n",
        "        'pretrained_model_checkpoint_dir': 'gamd_model_ckpt_dir/version_7/',\n",
        "        'model_param_config':{\n",
        "                  'in_feats' : 1 ,\n",
        "                 'encoding_size' : 128,\n",
        "                 'out_feats' : 3 ,\n",
        "                 'box_size' : 27.27 ,\n",
        "                 'bond' :None,       #\n",
        "                 'hidden_dim':128,\n",
        "                 'conv_layer':4,\n",
        "                 'edge_embedding_dim':128,\n",
        "                 'dropout':0.1,\n",
        "                 'drop_edge':True,\n",
        "                 'use_layer_norm': False,\n",
        "                            }\n",
        "        'model_hyperparam_config':{\n",
        "                                'loss_fn': 'constrained_edge_std',\n",
        "                                'lr': 0.00001,\n",
        "                                'reg_factor': 0.001,\n",
        "                                'n_epochs': 30,\n",
        "                                'batch_size': 1,\n",
        "                                }\n",
        "                },\n",
        "    'SR_config':{\n",
        "                'input_config':{\n",
        "                                'preprocessing_pipeline': ['avg_filter']\n",
        "                              },\n",
        "                'sr_regressor_config':{\n",
        "                                      'n_iter': 1000,\n",
        "                                      'unary_operators': [],\n",
        "                                      'binary_operators': (*, /),\n",
        "                                      'custom_operators': ,\n",
        "                                      'maxsize':25,\n",
        "                                      'batching':'True',\n",
        "                                      'template_expression':'TemplateExpression',\n",
        "                                      'custom_objective':'((x1), (y1)) -> abs(y1 - x1)',\n",
        "                                      'model_selection_criteria':'SymbolicRegression.MLJInterfaceModule.choose_best',\n",
        "                                      }\n",
        "              'sr_model_checkpoint_dir': 'sr_model_ckpt_dir/run_1',\n",
        "              },\n",
        "    'eval_config':{\n",
        "                  'metric': 'MAE',\n",
        "                  }\n",
        "}"
      ],
      "metadata": {
        "id": "4wsOYABaWXQg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}